# REV Model Configuration - Real Model Requirements
# This configuration specifies requirements for actual AI model execution (no mock data)

# System Requirements for Real Model Execution
system_requirements:
  min_ram_gb: 8
  recommended_ram_gb: 16
  min_storage_gb: 10
  recommended_storage_gb: 50
  python_version: ">=3.8"
  
  # GPU Requirements (optional but recommended)
  gpu:
    cuda_version: ">=11.0"
    min_vram_gb: 4
    recommended_vram_gb: 8
    compute_capability: ">=6.1"

# Real Model Paths and Configuration
model_sources:
  # Local model directories (actual transformer models required)
  local_models:
    base_path: "/Users/rohanvinaik/LLM_models"
    models:
      gpt2:
        path: "/Users/rohanvinaik/LLM_models/gpt2"
        memory_gb: 0.5
        parameters: 124000000
        architecture: "gpt"
      distilgpt2:
        path: "/Users/rohanvinaik/LLM_models/distilgpt2"
        memory_gb: 0.3
        parameters: 82000000
        architecture: "gpt"
      pythia-70m:
        path: "/Users/rohanvinaik/LLM_models/pythia-70m"
        memory_gb: 0.2
        parameters: 70000000
        architecture: "gpt"
      
  # Hugging Face Hub models (downloaded on first use)
  huggingface_models:
    gpt2: "gpt2"
    distilgpt2: "distilgpt2"
    pythia-70m: "EleutherAI/pythia-70m"
    pythia-160m: "EleutherAI/pythia-160m"
    gpt-neo-125m: "EleutherAI/gpt-neo-125M"
    
  # API-based models (requires API keys)
  api_models:
    openai:
      base_url: "https://api.openai.com/v1"
      supported_models:
        - "gpt-3.5-turbo"
        - "gpt-4"
        - "gpt-4-turbo"
      rate_limit_rpm: 60
      
    anthropic:
      base_url: "https://api.anthropic.com/v1"
      supported_models:
        - "claude-3-haiku-20240307"
        - "claude-3-sonnet-20240229"
        - "claude-3-opus-20240229"
      rate_limit_rpm: 50
      
    cohere:
      base_url: "https://api.cohere.ai/v1"
      supported_models:
        - "command"
        - "command-light"
      rate_limit_rpm: 100

# Memory Management Configuration
memory_management:
  segment_runner:
    max_memory_gb: 4.0
    buffer_size_segments: 4
    use_gradient_checkpointing: true
    precision: "fp16"  # or "fp32", "bf16"
    
  parallel_pipeline:
    max_workers: 4
    memory_per_worker_gb: 2.0
    use_gpu_when_available: true
    
  api_cache:
    max_entries: 1000
    ttl_seconds: 3600
    max_model_cache: 3

# Real Model Execution Settings
execution_settings:
  # NO MOCK DATA - All execution uses real models
  disable_mock_fallbacks: true
  require_real_models: true
  
  # Activation extraction configuration
  activation_extraction:
    layers_to_probe:
      - "transformer.h.0.attn.c_attn"
      - "transformer.h.1.attn.c_attn" 
      - "transformer.h.2.attn.c_attn"
      - "transformer.h.0.mlp.c_fc"
      - "transformer.h.1.mlp.c_fc"
      - "transformer.h.2.mlp.c_fc"
      - "transformer.ln_f"
    
    # Memory-efficient extraction
    cpu_offload: true
    gpu_cache_clear: true
    precision_mode: "mixed"  # fp16 for speed, fp32 for accuracy
    
  # Model loading configuration
  model_loading:
    low_cpu_mem_usage: true
    torch_dtype: "auto"  # Uses model's default precision
    device_map: "auto"  # Automatic device assignment
    trust_remote_code: false  # Security: only official models

# Verification Configuration
verification_settings:
  challenges:
    generation_method: "hmac_kdf"  # Cryptographically secure
    num_challenges: 100
    max_length: 512
    
  signatures:
    hash_algorithm: "sha256"
    include_metadata: true
    verify_uniqueness: true
    
  sequential_testing:
    alpha: 0.05  # Type I error rate
    beta: 0.10   # Type II error rate
    early_stopping: true

# Performance Monitoring
monitoring:
  log_level: "INFO"
  track_memory_usage: true
  track_gpu_utilization: true
  track_inference_time: true
  
  # Performance targets (for real model execution)
  performance_targets:
    max_memory_per_segment_gb: 2.0
    max_inference_time_ms: 1000
    min_throughput_tokens_per_sec: 100
    
  # Alert thresholds
  alerts:
    memory_usage_threshold: 0.85
    gpu_memory_threshold: 0.90
    inference_timeout_ms: 5000

# Security Configuration
security:
  # No mock data allowed in production
  allow_mock_responses: false
  require_signed_challenges: true
  verify_model_authenticity: true
  
  # API security
  api_key_validation: true
  rate_limiting: true
  request_timeout_seconds: 30

# Testing Configuration
testing:
  # Real model testing requirements
  require_actual_models: true
  test_model_loading: true
  test_activation_extraction: true
  test_signature_generation: true
  
  # Test models (must be available)
  test_models:
    - "gpt2"
    - "distilgpt2"
    
  # Performance validation
  validate_memory_usage: true
  validate_gpu_utilization: true
  validate_inference_speed: true

# Logging and Debugging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Log real model operations
  log_model_loading: true
  log_api_calls: true
  log_activation_extraction: true
  log_signature_generation: true
  
  # Performance logging
  log_memory_usage: true
  log_timing: true
  log_errors: true

# Development vs Production Settings
environments:
  development:
    allow_smaller_models: true
    reduced_memory_mode: true
    debug_logging: true
    
  production:
    strict_memory_limits: true
    require_gpu_for_large_models: true
    performance_monitoring: true
    security_validation: true