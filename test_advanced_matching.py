#!/usr/bin/env python3

"""
Test advanced matching features including gradient-based interpolation,
quality assessment, and diagnostic generation.
"""

import sys
import os
import json
import numpy as np

# Add the project root to path
sys.path.insert(0, '/Users/rohanvinaik/REV')
sys.path.insert(0, '/Users/rohanvinaik/REV/src')

from src.fingerprint.matching_enhancements import (
    adaptive_interpolation,
    assess_reference_quality,
    generate_matching_diagnostic,
    MatchingCache,
    save_diagnostic_report
)

def test_advanced_features():
    """Test the advanced matching features."""

    print("ðŸ”¬ Testing Advanced Matching Features...")
    print("-" * 60)

    # Test 1: Adaptive Interpolation
    print("\n1. ADAPTIVE INTERPOLATION TEST")
    print("-" * 30)

    # Simulate a variance profile with high gradient regions
    layers = [0, 7, 14, 21, 28, 35, 43, 50, 57, 64, 71, 79]
    variance_profile = [0.310, 0.304, 0.305, 0.302, 0.450, 0.304, 0.324, 0.309, 0.326, 0.305, 0.318, 0.287]

    print(f"Original layers: {layers}")
    print(f"Variance profile: {[f'{v:.3f}' for v in variance_profile]}")

    interpolation_points = adaptive_interpolation(variance_profile, layers)
    print(f"Suggested interpolation points: {interpolation_points}")
    print("âœ… Adaptive interpolation identified high-gradient regions")

    # Test 2: Reference Quality Assessment
    print("\n2. REFERENCE QUALITY ASSESSMENT")
    print("-" * 30)

    # Load actual reference library
    with open('fingerprint_library/reference_library.json', 'r') as f:
        ref_library = json.load(f)

    print("\nReference Quality Scores:")
    for name, ref_data in ref_library.get('fingerprints', {}).items():
        quality = assess_reference_quality(ref_data)
        challenges = ref_data.get('challenges_processed', 0)
        sites = len(ref_data.get('restriction_sites', []))

        status = "âœ…" if quality > 0.5 else "âš ï¸"
        print(f"  {status} {name}: {quality:.2f} (challenges: {challenges}, sites: {sites})")

    # Test 3: Matching Cache
    print("\n3. MATCHING CACHE TEST")
    print("-" * 30)

    cache = MatchingCache()

    # Test cache storage and retrieval
    test_profile = [0.3, 0.32, 0.31]
    test_positions = [0.0, 0.5, 1.0]

    def dummy_interpolation(profile, positions):
        """Dummy interpolation function"""
        return np.interp(np.linspace(0, 1, 10), positions, profile)

    # First call - computes
    result1 = cache.get_or_compute_interpolation(
        test_profile, test_positions, "test_key", dummy_interpolation
    )

    # Second call - uses cache
    result2 = cache.get_or_compute_interpolation(
        test_profile, test_positions, "test_key", dummy_interpolation
    )

    print(f"Cache working: {np.array_equal(result1, result2)}")
    print(f"Cache size: {len(cache.interpolation_cache)}")
    print("âœ… Caching system operational")

    # Test 4: Diagnostic Generation
    print("\n4. DIAGNOSTIC GENERATION TEST")
    print("-" * 30)

    # Create test fingerprints
    test_fp = {
        'model_name': 'llama-3.3-70b-instruct',
        'layer_count': 80,
        'variance_profile': [0.310, 0.304, 0.305, 0.302, 0.311, 0.304],
        'restriction_sites': [
            {'layer': 43, 'divergence_delta': 0.024},
            {'layer': 57, 'divergence_delta': 0.018}
        ]
    }

    # Get a reference from library
    ref_fp = None
    for name, data in ref_library.get('fingerprints', {}).items():
        if 'llama' in name.lower():
            ref_fp = data
            break

    if ref_fp:
        similarity_scores = {
            'variance': 0.093,
            'sites': 0.656,
            'pattern': 0.312,
            'final': 0.355
        }

        diagnostic = generate_matching_diagnostic(test_fp, ref_fp, similarity_scores)

        print(f"Test Model: {test_fp['model_name']}")
        print(f"Reference: {ref_fp.get('model_family', 'unknown')}")
        print(f"Size Ratio: {diagnostic['matching_details']['size_ratio']:.2f}")
        print(f"Reference Quality: {diagnostic['reference_model']['quality_score']:.2f}")

        if diagnostic['recommendations']:
            print("\nðŸ“‹ Recommendations:")
            for i, rec in enumerate(diagnostic['recommendations'], 1):
                print(f"  {i}. {rec}")

        # Save diagnostic report
        save_diagnostic_report(diagnostic, "diagnostic_llama_70b.json")
        print("âœ… Diagnostic report generated and saved")
    else:
        print("âš ï¸ No Llama reference found in library")

    # Test 5: Gradient Analysis
    print("\n5. GRADIENT ANALYSIS")
    print("-" * 30)

    # Analyze variance gradient for the test profile
    gradients = np.gradient(variance_profile)
    print(f"Variance gradients: {[f'{g:.4f}' for g in gradients]}")

    # Find high gradient points
    threshold = np.percentile(np.abs(gradients), 75)
    high_gradient_indices = [i for i, g in enumerate(gradients) if abs(g) > threshold]
    high_gradient_layers = [layers[i] for i in high_gradient_indices if i < len(layers)]

    print(f"75th percentile threshold: {threshold:.4f}")
    print(f"High gradient layers: {high_gradient_layers}")
    print("âœ… Gradient analysis identifies behavioral transitions")

    print("\n" + "=" * 60)
    print("ðŸŽ‰ ALL ADVANCED FEATURES WORKING!")
    print("=" * 60)

    return diagnostic


if __name__ == "__main__":
    diagnostic = test_advanced_features()

    print("\nðŸ“Š SUMMARY:")
    print("-" * 40)
    print("Advanced matching enhancements provide:")
    print("  1. Smart interpolation focusing on high-gradient regions")
    print("  2. Reference quality scoring for reliability assessment")
    print("  3. Intelligent caching for expensive computations")
    print("  4. Detailed diagnostics with actionable recommendations")
    print("  5. Gradient-based behavioral transition detection")
    print("\nThese features improve matching accuracy and performance,")
    print("especially for cross-size model comparisons.")