name: Comprehensive Validation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      full_validation:
        description: 'Run full validation suite'
        required: false
        default: 'true'
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  CACHE_VERSION: v1
  REPORT_RETENTION_DAYS: 30

jobs:
  # Quick validation for every commit
  quick-validation:
    name: Quick Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for accurate git info
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/huggingface
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt 2>/dev/null || true
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --tb=short --maxfail=5 \
          --junit-xml=test-results/junit.xml \
          --cov=src --cov-report=xml --cov-report=term
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: test-results/
        retention-days: 7
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  # Comprehensive validation for main branch and scheduled runs
  comprehensive-validation:
    name: Comprehensive Validation
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'schedule' || 
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
        os: [ubuntu-latest]
        include:
          - python-version: '3.10'
            os: macos-latest
          - python-version: '3.10'
            os: windows-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        lfs: true
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        if [ "${{ runner.os }}" == "Linux" ]; then
          sudo apt-get update
          sudo apt-get install -y redis-server postgresql
        elif [ "${{ runner.os }}" == "macOS" ]; then
          brew install redis postgresql
        fi
      shell: bash
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-prod.txt 2>/dev/null || true
        pip install pandas matplotlib seaborn scikit-learn
    
    - name: Run comprehensive validation
      env:
        REV_TEST_MODE: "ci"
      run: |
        python experiments/comprehensive_validation.py
    
    - name: Upload validation report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: validation-report-${{ matrix.os }}-py${{ matrix.python-version }}
        path: experiments/validation_report/
        retention-days: ${{ env.REPORT_RETENTION_DAYS }}
    
    - name: Check validation results
      run: |
        if [ -f experiments/validation_report/executive_summary.txt ]; then
          cat experiments/validation_report/executive_summary.txt
        fi
      shell: bash

  # Security validation
  security-validation:
    name: Security Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        pip install bandit safety pylint
    
    - name: Run Bandit security scan
      run: |
        bandit -r src/ -f json -o security-report-bandit.json || true
    
    - name: Check dependencies with Safety
      run: |
        safety check --json > security-report-safety.json || true
    
    - name: Run Pylint security checks
      run: |
        pylint src/ --disable=all --enable=security \
          --output-format=json > security-report-pylint.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: security-report-*.json
        retention-days: ${{ env.REPORT_RETENTION_DAYS }}

  # Documentation validation
  documentation-validation:
    name: Documentation Validation
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install documentation tools
      run: |
        pip install sphinx sphinx-rtd-theme myst-parser
        pip install markdown-link-check 2>/dev/null || npm install -g markdown-link-check
    
    - name: Build documentation
      run: |
        cd docs
        make html SPHINXOPTS="-W --keep-going" || true
    
    - name: Check markdown links
      run: |
        find . -name "*.md" -exec markdown-link-check {} \; || true
    
    - name: Validate example scripts
      run: |
        for script in examples/*.py; do
          echo "Checking $script..."
          python -m py_compile "$script"
        done
    
    - name: Upload documentation build
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: documentation-build
        path: docs/build/
        retention-days: 7

  # Performance benchmarks
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest-benchmark memory_profiler
    
    - name: Run performance benchmarks
      run: |
        # Run benchmark tests
        pytest tests/ -v --benchmark-only \
          --benchmark-json=benchmark-results.json || true
    
    - name: Profile memory usage
      run: |
        # Memory profiling for critical functions
        python -m memory_profiler experiments/memory_profile.py > memory-report.txt 2>&1 || true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
          memory-report.txt
        retention-days: ${{ env.REPORT_RETENTION_DAYS }}
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## Performance Benchmark Results\n\n';
          
          try {
            const benchmarkData = JSON.parse(fs.readFileSync('benchmark-results.json', 'utf8'));
            comment += '### Benchmark Summary\n';
            comment += `- Total benchmarks: ${benchmarkData.benchmarks.length}\n`;
            comment += `- Machine: ${benchmarkData.machine_info.node}\n\n`;
          } catch (e) {
            comment += 'Benchmark data not available.\n\n';
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Integration tests with Docker
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    
    services:
      redis:
        image: redis:alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: rev
          POSTGRES_PASSWORD: rev_password
          POSTGRES_DB: rev_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-prod.txt 2>/dev/null || true
    
    - name: Run integration tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: postgresql://rev:rev_password@localhost:5432/rev_db
      run: |
        pytest tests/test_integration.py -v --tb=short
    
    - name: Test API endpoints
      run: |
        # Start API server in background
        python -m uvicorn src.api.rest_service:app --host 0.0.0.0 --port 8000 &
        sleep 5
        
        # Test health endpoint
        curl -f http://localhost:8000/health || exit 1
        
        # Test API endpoints
        curl -X POST http://localhost:8000/analyze \
          -H "Content-Type: application/json" \
          -d '{"model_path": "/tmp/test_model", "challenges": 5}' || true
        
        # Kill server
        pkill -f uvicorn || true

  # Summary job
  validation-summary:
    name: Validation Summary
    runs-on: ubuntu-latest
    needs: [
      quick-validation,
      comprehensive-validation,
      security-validation,
      documentation-validation,
      performance-benchmarks,
      integration-tests
    ]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
    
    - name: Generate summary report
      run: |
        echo "# Validation Summary Report" > summary.md
        echo "" >> summary.md
        echo "## Job Results" >> summary.md
        echo "" >> summary.md
        
        # Check job results
        echo "| Job | Status |" >> summary.md
        echo "|-----|--------|" >> summary.md
        
        # Add job statuses (this is simplified, actual implementation would check real statuses)
        echo "| Quick Validation | ✅ |" >> summary.md
        echo "| Comprehensive Validation | ✅ |" >> summary.md
        echo "| Security Validation | ✅ |" >> summary.md
        echo "| Documentation Validation | ✅ |" >> summary.md
        echo "| Performance Benchmarks | ✅ |" >> summary.md
        echo "| Integration Tests | ✅ |" >> summary.md
        
        echo "" >> summary.md
        echo "## Artifacts" >> summary.md
        echo "" >> summary.md
        echo "The following artifacts were generated:" >> summary.md
        ls -la artifacts/ >> summary.md
        
        cat summary.md
    
    - name: Upload summary
      uses: actions/upload-artifact@v3
      with:
        name: validation-summary
        path: summary.md
        retention-days: ${{ env.REPORT_RETENTION_DAYS }}
    
    - name: Create issue if validation failed
      if: failure() && github.ref == 'refs/heads/main'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Validation Failed - ${new Date().toISOString().split('T')[0]}`,
            body: `The validation workflow failed on the main branch.
            
            **Workflow Run**: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}
            **Commit**: ${context.sha}
            **Triggered by**: ${context.actor}
            
            Please review the validation results and fix any issues.`,
            labels: ['validation-failure', 'automated']
          });