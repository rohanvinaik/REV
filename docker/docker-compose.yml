version: '3.8'

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    labels: "service"

x-security-opts: &security-opts
  - no-new-privileges:true
  - apparmor:docker-default

services:
  # ==================== CORE SERVICES ====================
  
  # REV Verifier Service - 3 replicas for fast verification
  rev-verifier:
    build:
      context: ..
      dockerfile: docker/Dockerfile.rev-verifier
      args:
        - BUILD_DATE=${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        - VERSION=${VERSION:-latest}
    image: rev/verifier:${VERSION:-latest}
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 4096M
          cpus: '2.0'
        reservations:
          memory: 2048M
          cpus: '1.0'
          devices:
            - capabilities: [gpu]
              count: 1  # Optional GPU support
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
    user: "1000:1000"  # Non-root user
    security_opt: *security-opts
    read_only: true  # Read-only root filesystem
    tmpfs:
      - /tmp:noexec,nosuid,size=100M
      - /run:noexec,nosuid,size=10M
    environment:
      - SERVICE_NAME=rev-verifier
      - MAX_MEMORY_MB=4096
      - SEGMENT_SIZE=512
      - BUFFER_SIZE=4
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - REDIS_URL=redis://redis:6379/0
      - METRICS_PORT=9100
      - CHECKPOINT_DIR=/data/checkpoints
      - CACHE_DIR=/data/cache
      - TLS_ENABLED=${TLS_ENABLED:-true}
      - TLS_CERT_PATH=/certs/server.crt
      - TLS_KEY_PATH=/certs/server.key
      - RATE_LIMIT_RPS=100
      - RATE_LIMIT_BURST=200
      - JAEGER_AGENT_HOST=jaeger
      - JAEGER_AGENT_PORT=6831
    env_file:
      - ../config/.env.production
    secrets:
      - rev_api_key
      - db_password
    ports:
      - target: 8001
        published: 8001-8003
        protocol: tcp
        mode: host
    networks:
      - rev-backend
      - monitoring
    volumes:
      - type: volume
        source: checkpoint-data
        target: /data/checkpoints
      - type: volume
        source: cache-data
        target: /data/cache
      - type: bind
        source: ./certs
        target: /certs
        read_only: true
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "https://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.rev-verifier.rule=PathPrefix(`/api/verify`)"
      - "traefik.http.services.rev-verifier.loadbalancer.server.port=8001"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9100"

  # HBT Consensus Service - 5 replicas for Byzantine fault tolerance
  hbt-consensus:
    build:
      context: ..
      dockerfile: docker/Dockerfile.hbt-consensus
      args:
        - BUILD_DATE=${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        - VERSION=${VERSION:-latest}
    image: rev/hbt-consensus:${VERSION:-latest}
    deploy:
      replicas: 5
      resources:
        limits:
          memory: 8192M
          cpus: '4.0'
        reservations:
          memory: 4096M
          cpus: '2.0'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      placement:
        constraints:
          - node.labels.consensus == true
        preferences:
          - spread: node.id  # Spread across different nodes
    user: "1000:1000"
    security_opt: *security-opts
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=200M
    environment:
      - SERVICE_NAME=hbt-consensus
      - MAX_MEMORY_MB=8192
      - NUM_VALIDATORS=5
      - FAULT_TOLERANCE=1
      - CONSENSUS_THRESHOLD=0.67
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - REDIS_URL=redis://redis:6379/1
      - METRICS_PORT=9101
      - CHECKPOINT_DIR=/data/consensus
      - TLS_ENABLED=${TLS_ENABLED:-true}
      - PEER_DISCOVERY=consul
      - CONSUL_URL=http://consul:8500
    env_file:
      - ../config/.env.production
    secrets:
      - consensus_key
      - validator_keys
    ports:
      - target: 8002
        published: 8004-8008
        protocol: tcp
        mode: host
    networks:
      - rev-backend
      - consensus-network
      - monitoring
    volumes:
      - type: volume
        source: consensus-data
        target: /data/consensus
      - type: bind
        source: ./certs
        target: /certs
        read_only: true
    depends_on:
      redis:
        condition: service_healthy
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "https://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    labels:
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9101"

  # Unified Coordinator Service - 2 replicas for HA
  unified-coordinator:
    build:
      context: ..
      dockerfile: docker/Dockerfile.unified-coordinator
      args:
        - BUILD_DATE=${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        - VERSION=${VERSION:-latest}
    image: rev/unified-coordinator:${VERSION:-latest}
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 2048M
          cpus: '1.0'
        reservations:
          memory: 1024M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    user: "1000:1000"
    security_opt: *security-opts
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=50M
    environment:
      - SERVICE_NAME=unified-coordinator
      - MAX_MEMORY_MB=2048
      - CACHE_ENABLED=true
      - MODE_SELECTION=auto
      - REV_SERVICE_URL=http://rev-verifier:8001
      - HBT_SERVICE_URL=http://hbt-consensus:8002
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - REDIS_URL=redis://redis:6379/2
      - METRICS_PORT=9102
      - TLS_ENABLED=${TLS_ENABLED:-true}
      - RATE_LIMIT_RPS=500
      - CIRCUIT_BREAKER_THRESHOLD=0.5
      - CIRCUIT_BREAKER_TIMEOUT=60
    env_file:
      - ../config/.env.production
    secrets:
      - api_secret
      - jwt_secret
    ports:
      - "8000:8000"
    networks:
      - rev-backend
      - rev-frontend
      - monitoring
    depends_on:
      rev-verifier:
        condition: service_healthy
      hbt-consensus:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "https://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.coordinator.rule=Host(`api.rev.example.com`)"
      - "traefik.http.services.coordinator.loadbalancer.server.port=8000"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9102"

  # ==================== DATA STORES ====================

  # Redis for caching and message passing
  redis:
    image: redis:7-alpine
    deploy:
      resources:
        limits:
          memory: 2048M
          cpus: '1.0'
        reservations:
          memory: 1024M
          cpus: '0.5'
    user: "999:999"
    security_opt: *security-opts
    ports:
      - "6379:6379"
    networks:
      - rev-backend
    volumes:
      - redis-data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    logging: *default-logging

  # PostgreSQL for persistent storage
  postgres:
    image: postgres:15-alpine
    deploy:
      resources:
        limits:
          memory: 2048M
          cpus: '1.0'
        reservations:
          memory: 1024M
          cpus: '0.5'
    user: "999:999"
    security_opt: *security-opts
    environment:
      - POSTGRES_DB=rev_db
      - POSTGRES_USER=rev_user
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256 --auth-local=scram-sha-256
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
    secrets:
      - db_password
    ports:
      - "5432:5432"
    networks:
      - rev-backend
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rev_user -d rev_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *default-logging

  # ==================== SERVICE DISCOVERY ====================

  # Consul for service discovery
  consul:
    image: consul:latest
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    user: "100:1000"
    security_opt: *security-opts
    ports:
      - "8500:8500"
      - "8600:8600/udp"
    networks:
      - rev-backend
    volumes:
      - consul-data:/consul/data
      - ./consul:/consul/config:ro
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 10s
      timeout: 3s
      retries: 3
    logging: *default-logging

  # ==================== LOAD BALANCING & ROUTING ====================

  # Traefik as reverse proxy and load balancer
  traefik:
    image: traefik:v2.10
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    user: "1000:1000"
    security_opt: *security-opts
    read_only: true
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Dashboard (should be disabled in production)
    networks:
      - rev-frontend
      - rev-backend
      - monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ./traefik/dynamic:/etc/traefik/dynamic:ro
      - ./certs:/certs:ro
      - traefik-acme:/acme
    environment:
      - CF_API_EMAIL=${CF_API_EMAIL}
      - CF_DNS_API_TOKEN_FILE=/run/secrets/cf_api_token
    secrets:
      - cf_api_token
    command:
      - --api.dashboard=true
      - --api.debug=false
      - --log.level=${LOG_LEVEL:-INFO}
      - --accesslog=true
      - --metrics.prometheus=true
      - --metrics.prometheus.buckets=0.1,0.3,1.2,5.0
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --providers.file.directory=/etc/traefik/dynamic
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --certificatesresolvers.letsencrypt.acme.dnschallenge=true
      - --certificatesresolvers.letsencrypt.acme.dnschallenge.provider=cloudflare
      - --certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}
      - --certificatesresolvers.letsencrypt.acme.storage=/acme/acme.json
    healthcheck:
      test: ["CMD", "traefik", "healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    labels:
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=8080"

  # ==================== MONITORING STACK ====================

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    deploy:
      resources:
        limits:
          memory: 2048M
          cpus: '1.0'
        reservations:
          memory: 1024M
          cpus: '0.5'
    user: "65534:65534"  # nobody user
    security_opt: *security-opts
    read_only: true
    ports:
      - "9090:9090"
    networks:
      - monitoring
      - rev-backend
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    depends_on:
      - node-exporter
      - cadvisor

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    deploy:
      resources:
        limits:
          memory: 1024M
          cpus: '0.5'
    user: "472:472"
    security_opt: *security-opts
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=10M
    ports:
      - "3000:3000"
    networks:
      - monitoring
      - rev-frontend
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana/notifiers:/etc/grafana/provisioning/notifiers:ro
    environment:
      - GF_SECURITY_ADMIN_USER_FILE=/run/secrets/grafana_admin_user
      - GF_SECURITY_ADMIN_PASSWORD_FILE=/run/secrets/grafana_admin_password
      - GF_INSTALL_PLUGINS=redis-datasource,grafana-piechart-panel,grafana-clock-panel
      - GF_SERVER_ROOT_URL=https://grafana.rev.example.com
      - GF_SERVER_ENABLE_GZIP=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
      - GF_SECURITY_CONTENT_SECURITY_POLICY=true
    secrets:
      - grafana_admin_user
      - grafana_admin_password
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.rev.example.com`)"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

  # AlertManager for alert routing
  alertmanager:
    image: prom/alertmanager:latest
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
    user: "65534:65534"
    security_opt: *security-opts
    read_only: true
    ports:
      - "9093:9093"
    networks:
      - monitoring
    volumes:
      - ./alertmanager/config.yml:/etc/alertmanager/config.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
      - '--cluster.advertise-address=0.0.0.0:9093'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # Node Exporter for host metrics
  node-exporter:
    image: prom/node-exporter:latest
    deploy:
      mode: global  # Run on every node
      resources:
        limits:
          memory: 128M
          cpus: '0.1'
    user: "65534:65534"
    security_opt: *security-opts
    read_only: true
    ports:
      - "9100:9100"
    networks:
      - monitoring
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    deploy:
      mode: global
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
    security_opt: *security-opts
    read_only: true
    ports:
      - "8081:8080"
    networks:
      - monitoring
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true  # Required for container metrics
    command:
      - '--housekeeping_interval=30s'
      - '--docker_only=true'
      - '--store_container_labels=false'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # ==================== LOGGING STACK ====================

  # Loki for log aggregation
  loki:
    image: grafana/loki:latest
    deploy:
      resources:
        limits:
          memory: 1024M
          cpus: '0.5'
    user: "10001:10001"
    security_opt: *security-opts
    read_only: true
    ports:
      - "3100:3100"
    networks:
      - monitoring
    volumes:
      - ./loki/config.yml:/etc/loki/config.yml:ro
      - loki-data:/loki
    command: -config.file=/etc/loki/config.yml
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # Promtail for log shipping
  promtail:
    image: grafana/promtail:latest
    deploy:
      mode: global
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
    user: "0:0"  # Needs root to read logs
    security_opt: *security-opts
    read_only: true
    networks:
      - monitoring
    volumes:
      - ./promtail/config.yml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    logging: *default-logging

  # ==================== TRACING ====================

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    deploy:
      resources:
        limits:
          memory: 1024M
          cpus: '0.5'
    user: "10001:10001"
    security_opt: *security-opts
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    ports:
      - "5775:5775/udp"   # Zipkin compact thrift
      - "6831:6831/udp"   # Jaeger compact thrift
      - "6832:6832/udp"   # Jaeger binary thrift
      - "5778:5778"       # Serving frontend
      - "16686:16686"     # Jaeger UI
      - "14268:14268"     # Jaeger collector HTTP
      - "14250:14250"     # Jaeger gRPC
      - "9411:9411"       # Zipkin compatible endpoint
    networks:
      - monitoring
    volumes:
      - jaeger-data:/badger
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:14269/"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.rev.example.com`)"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"

# ==================== NETWORKS ====================

networks:
  rev-frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/24
    driver_opts:
      com.docker.network.bridge.name: br-rev-front

  rev-backend:
    driver: bridge
    internal: true  # No external access
    ipam:
      config:
        - subnet: 172.28.1.0/24
    driver_opts:
      com.docker.network.bridge.name: br-rev-back

  consensus-network:
    driver: overlay
    internal: true
    encrypted: true  # Encrypt consensus traffic
    ipam:
      config:
        - subnet: 172.28.2.0/24

  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.3.0/24
    driver_opts:
      com.docker.network.bridge.name: br-monitoring

# ==================== VOLUMES ====================

volumes:
  # Data volumes
  redis-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/redis

  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/postgres

  checkpoint-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/checkpoints

  cache-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/cache

  consensus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/consensus

  consul-data:
    driver: local

  # Monitoring volumes
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/prometheus

  grafana-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/grafana

  alertmanager-data:
    driver: local

  loki-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/loki

  jaeger-data:
    driver: local

  # Certificate volume
  traefik-acme:
    driver: local

# ==================== SECRETS ====================

secrets:
  rev_api_key:
    file: ../secrets/rev_api_key.txt
  
  db_password:
    file: ../secrets/db_password.txt
  
  consensus_key:
    file: ../secrets/consensus_key.txt
  
  validator_keys:
    file: ../secrets/validator_keys.json
  
  api_secret:
    file: ../secrets/api_secret.txt
  
  jwt_secret:
    file: ../secrets/jwt_secret.txt
  
  cf_api_token:
    file: ../secrets/cf_api_token.txt
  
  grafana_admin_user:
    file: ../secrets/grafana_admin_user.txt
  
  grafana_admin_password:
    file: ../secrets/grafana_admin_password.txt