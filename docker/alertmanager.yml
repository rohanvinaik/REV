global:
  # SMTP configuration for email alerts
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'rev-alerts@example.com'
  smtp_auth_username: 'rev-alerts@example.com'
  smtp_auth_password_file: '/etc/alertmanager/smtp_password'
  smtp_require_tls: true

  # Slack configuration
  slack_api_url_file: '/etc/alertmanager/slack_webhook'

  # PagerDuty configuration
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

  # Global settings
  resolve_timeout: 5m
  http_config:
    tls_config:
      insecure_skip_verify: false

# Templates for alert messages
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  # Default receiver
  receiver: 'default'
  
  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'service', 'severity']
  
  # Wait before sending grouped alerts
  group_wait: 30s
  
  # Wait before sending new alerts for a group
  group_interval: 5m
  
  # Wait before re-sending alerts
  repeat_interval: 12h

  # Child routes for specific routing
  routes:
    # Critical alerts go to PagerDuty and Slack
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      group_interval: 1m
      repeat_interval: 1h
      continue: true

    # Consensus-specific alerts
    - match:
        component: consensus
      receiver: 'consensus-team'
      group_by: ['alertname', 'consensus_node_id']
      routes:
        - match:
            alertname: ByzantineFaultDetected
          receiver: 'security-team'
          group_wait: 0s
          repeat_interval: 5m

    # Security alerts
    - match:
        component: security
      receiver: 'security-team'
      group_wait: 0s
      group_interval: 1m
      repeat_interval: 5m

    # Performance alerts
    - match:
        component: performance
      receiver: 'performance-team'
      group_interval: 10m
      repeat_interval: 1h

    # SLO violations
    - match_re:
        slo: '.+'
      receiver: 'slo-violations'
      group_by: ['slo']
      group_wait: 1m
      repeat_interval: 30m

    # Infrastructure alerts
    - match:
        component: infrastructure
      receiver: 'ops-team'
      routes:
        - match:
            alertname: DiskSpaceRunningLow
          receiver: 'ops-urgent'
          repeat_interval: 30m
        - match:
            alertname: CertificateExpiringSoon
          receiver: 'ops-team'
          repeat_interval: 24h

    # Database and cache alerts
    - match_re:
        component: '(database|cache|messaging)'
      receiver: 'data-team'
      group_interval: 10m

    # HDC system alerts
    - match:
        component: hdc
      receiver: 'ml-team'
      group_by: ['alertname']

    # GPU-related alerts
    - match:
        component: gpu
      receiver: 'gpu-team'
      group_by: ['gpu_id', 'node']

# Receivers configuration
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-logger:8080/alerts'
        send_resolved: true

  - name: 'critical-alerts'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/pagerduty_key'
        description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'critical'
        client: 'REV AlertManager'
        client_url: 'https://rev.example.com/alerts'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'üö® Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        actions:
          - type: button
            text: 'View in Grafana'
            url: 'https://grafana.rev.example.com'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '[CRITICAL] REV Alert: {{ .GroupLabels.alertname }}'

  - name: 'consensus-team'
    slack_configs:
      - channel: '#consensus-alerts'
        title: 'Consensus Alert: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
        send_resolved: true
    email_configs:
      - to: 'consensus-team@example.com'

  - name: 'security-team'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/security_pagerduty_key'
        severity: 'critical'
    slack_configs:
      - channel: '#security-alerts'
        title: 'üîí Security Alert'
        text: '{{ .CommonAnnotations.description }}'
        send_resolved: false
    email_configs:
      - to: 'security@example.com'
        headers:
          Subject: '[SECURITY] {{ .GroupLabels.alertname }}'
          X-Priority: '1'

  - name: 'performance-team'
    slack_configs:
      - channel: '#performance'
        title: 'Performance Alert'
        text: '{{ .CommonAnnotations.summary }}'
        send_resolved: true

  - name: 'slo-violations'
    slack_configs:
      - channel: '#slo-alerts'
        title: 'üìä SLO Violation: {{ .GroupLabels.slo }}'
        text: '{{ .CommonAnnotations.description }}'
        color: 'danger'
    email_configs:
      - to: 'slo-team@example.com'
        send_resolved: true

  - name: 'ops-team'
    slack_configs:
      - channel: '#ops'
        title: 'Infrastructure Alert'
        text: '{{ .CommonAnnotations.summary }}'
    email_configs:
      - to: 'ops@example.com'

  - name: 'ops-urgent'
    pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/ops_pagerduty_key'
        severity: 'warning'
    slack_configs:
      - channel: '#ops-urgent'
        title: '‚ö†Ô∏è Urgent Ops Alert'
        text: '{{ .CommonAnnotations.description }}'

  - name: 'data-team'
    slack_configs:
      - channel: '#data-platform'
        title: 'Data Platform Alert: {{ .GroupLabels.component }}'
        text: '{{ .CommonAnnotations.description }}'

  - name: 'ml-team'
    slack_configs:
      - channel: '#ml-systems'
        title: 'HDC System Alert'
        text: '{{ .CommonAnnotations.summary }}\n{{ .CommonAnnotations.description }}'
    email_configs:
      - to: 'ml-team@example.com'

  - name: 'gpu-team'
    slack_configs:
      - channel: '#gpu-cluster'
        title: 'GPU Alert: {{ .GroupLabels.gpu_id }}'
        text: 'Node: {{ .GroupLabels.node }}\n{{ .CommonAnnotations.description }}'
    webhook_configs:
      - url: 'http://gpu-monitor:8080/alerts'

# Inhibition rules to suppress certain alerts
inhibit_rules:
  # If a service is down, inhibit all other alerts for that service
  - source_match:
      alertname: 'REVServiceDown'
    target_match_re:
      alertname: '.+'
    equal: ['service', 'instance']

  # If consensus is completely down, inhibit agreement rate alerts
  - source_match:
      alertname: 'ConsensusNodeDown'
      severity: 'critical'
    target_match:
      alertname: 'LowConsensusAgreement'
    equal: ['cluster']

  # If database is down, inhibit pool exhaustion alerts
  - source_match:
      alertname: 'DatabaseDown'
    target_match:
      alertname: 'DatabaseConnectionPoolExhausted'
    equal: ['instance']

  # If there's a Byzantine fault, inhibit normal consensus alerts
  - source_match:
      alertname: 'ByzantineFaultDetected'
    target_match_re:
      component: 'consensus'
      severity: 'warning'
    equal: ['cluster']

  # If disk space is critical, inhibit warning level alerts
  - source_match:
      alertname: 'DiskSpaceCritical'
    target_match:
      alertname: 'DiskSpaceRunningLow'
    equal: ['instance', 'mountpoint']

# Mute time intervals (optional)
mute_time_intervals:
  - name: 'weekends'
    time_intervals:
      - weekdays: ['saturday', 'sunday']
  
  - name: 'nights'
    time_intervals:
      - times:
          - start_time: '22:00'
            end_time: '06:00'
  
  - name: 'maintenance'
    time_intervals:
      - weekdays: ['sunday']
        times:
          - start_time: '02:00'
            end_time: '04:00'