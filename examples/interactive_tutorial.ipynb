{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REV System Interactive Tutorial\n",
    "\n",
    "This interactive notebook demonstrates the REV (Restriction Enzyme Verification) System capabilities.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic Model Verification](#basic)\n",
    "3. [Feature Extraction](#features)\n",
    "4. [Hyperdimensional Fingerprints](#fingerprints)\n",
    "5. [Prompt Orchestration](#orchestration)\n",
    "6. [Memory-Bounded Execution](#memory)\n",
    "7. [Statistical Testing](#statistics)\n",
    "8. [Advanced Analysis](#advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add REV to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import REV modules\n",
    "from run_rev import REVUnified\n",
    "from src.features.taxonomy import HierarchicalFeatureTaxonomy\n",
    "from src.hdc.encoder import HDCEncoder\n",
    "from src.core.sequential import SequentialTest\n",
    "from src.orchestration.prompt_orchestrator import PromptOrchestrator\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ REV System loaded successfully!\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available models\n",
    "def find_models():\n",
    "    \"\"\"Find available models on the system.\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # Common model locations\n",
    "    search_paths = [\n",
    "        Path.home() / \"LLM_models\",\n",
    "        Path.home() / \".cache\" / \"huggingface\" / \"hub\",\n",
    "        Path(\"/opt/models\"),\n",
    "        Path(\"/mnt/models\")\n",
    "    ]\n",
    "    \n",
    "    for path in search_paths:\n",
    "        if path.exists():\n",
    "            # Look for config.json files\n",
    "            configs = list(path.glob(\"*/config.json\")) + \\\n",
    "                     list(path.glob(\"*/snapshots/*/config.json\"))\n",
    "            \n",
    "            for config in configs:\n",
    "                model_dir = config.parent\n",
    "                models.append({\n",
    "                    'name': model_dir.name,\n",
    "                    'path': str(model_dir),\n",
    "                    'size_gb': sum(f.stat().st_size for f in model_dir.rglob('*')) / (1024**3)\n",
    "                })\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = find_models()\n",
    "print(f\"Found {len(models)} models:\\n\")\n",
    "\n",
    "for i, model in enumerate(models[:5], 1):  # Show first 5\n",
    "    print(f\"{i}. {model['name']}\")\n",
    "    print(f\"   Path: {model['path']}\")\n",
    "    print(f\"   Size: {model['size_gb']:.1f} GB\\n\")\n",
    "\n",
    "# Select a model for examples\n",
    "if models:\n",
    "    MODEL_PATH = models[0]['path']\n",
    "    print(f\"üì¶ Using model: {models[0]['name']}\")\n",
    "else:\n",
    "    MODEL_PATH = \"/path/to/your/model\"\n",
    "    print(\"‚ö†Ô∏è  No models found. Please set MODEL_PATH manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Model Verification <a id='basic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize REV system\n",
    "rev = REVUnified(\n",
    "    memory_limit_gb=4.0,\n",
    "    debug=False  # Set True for verbose output\n",
    ")\n",
    "\n",
    "print(\"üöÄ REV System initialized\")\n",
    "print(f\"   Memory limit: 4.0 GB\")\n",
    "print(f\"   Device: auto-detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run basic verification (quick test with 5 challenges)\n",
    "print(f\"üî¨ Verifying model: {MODEL_PATH}\\n\")\n",
    "\n",
    "result = rev.process_model(MODEL_PATH, challenges=5)\n",
    "\n",
    "# Display results\n",
    "print(\"‚úÖ Verification Complete!\\n\")\n",
    "print(f\"Model Family: {result.get('model_family', 'Unknown')}\")\n",
    "print(f\"Confidence: {result.get('confidence', 0):.2%}\")\n",
    "print(f\"Architecture: {result.get('architecture', 'Unknown')}\")\n",
    "\n",
    "# Cleanup\n",
    "rev.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction <a id='features'></a>\n",
    "\n",
    "REV uses 56 principled features across 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature taxonomy\n",
    "taxonomy = HierarchicalFeatureTaxonomy()\n",
    "\n",
    "# Example text for feature extraction\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This sentence contains\n",
    "every letter of the alphabet. Machine learning models process text\n",
    "by converting it into numerical representations.\n",
    "\"\"\"\n",
    "\n",
    "# Extract features\n",
    "features = taxonomy.extract_all_features(\n",
    "    model_output=sample_text,\n",
    "    prompt=\"Describe machine learning\"\n",
    ")\n",
    "\n",
    "# Display feature counts\n",
    "print(\"üìä Feature Categories:\\n\")\n",
    "for category, values in features.items():\n",
    "    print(f\"{category.capitalize():15} {len(values):3} features\")\n",
    "\n",
    "print(f\"\\nTotal features: {sum(len(v) for v in features.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (category, values) in zip(axes.flat, features.items()):\n",
    "    ax.bar(range(len(values)), values, color=f'C{list(features.keys()).index(category)}')\n",
    "    ax.set_title(f'{category.capitalize()} Features')\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions by Category', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nüìà Feature Statistics:\")\n",
    "for category, values in features.items():\n",
    "    print(f\"\\n{category.capitalize()}:\")\n",
    "    print(f\"  Mean: {np.mean(values):.3f}\")\n",
    "    print(f\"  Std:  {np.std(values):.3f}\")\n",
    "    print(f\"  Range: [{np.min(values):.3f}, {np.max(values):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperdimensional Fingerprints <a id='fingerprints'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDC encoder\n",
    "encoder = HDCEncoder(\n",
    "    dimension=10000,\n",
    "    sparsity=0.01\n",
    ")\n",
    "\n",
    "print(\"üß¨ HDC Encoder Configuration:\")\n",
    "print(f\"  Dimension: {encoder.dimension:,}\")\n",
    "print(f\"  Sparsity: {encoder.sparsity:.1%}\")\n",
    "print(f\"  Expected ones: ~{int(encoder.dimension * encoder.sparsity)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode features to hypervector\n",
    "concat_features = taxonomy.get_concatenated_features(features)\n",
    "hypervector = encoder.encode_vector(concat_features)\n",
    "\n",
    "print(f\"‚úÖ Hypervector generated\")\n",
    "print(f\"  Shape: {hypervector.shape}\")\n",
    "print(f\"  Actual sparsity: {np.mean(hypervector):.3%}\")\n",
    "print(f\"  Number of ones: {np.sum(hypervector)}\")\n",
    "\n",
    "# Visualize hypervector pattern\n",
    "plt.figure(figsize=(14, 3))\n",
    "\n",
    "# Show first 1000 bits as image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(hypervector[:1000].reshape(20, 50), cmap='binary', aspect='auto')\n",
    "plt.title('First 1000 bits (20x50)')\n",
    "plt.xlabel('Bit Index')\n",
    "plt.ylabel('Row')\n",
    "\n",
    "# Histogram of segments\n",
    "plt.subplot(1, 3, 2)\n",
    "segment_sums = [np.sum(hypervector[i:i+100]) for i in range(0, len(hypervector), 100)]\n",
    "plt.hist(segment_sums, bins=20, color='blue', alpha=0.7)\n",
    "plt.xlabel('Ones per 100-bit segment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sparsity Distribution')\n",
    "\n",
    "# Cumulative distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "cumsum = np.cumsum(hypervector)\n",
    "plt.plot(cumsum, color='green', linewidth=1)\n",
    "plt.xlabel('Bit Position')\n",
    "plt.ylabel('Cumulative Ones')\n",
    "plt.title('Cumulative Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Two Hypervectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate second hypervector with slightly different features\n",
    "features2 = features.copy()\n",
    "# Perturb features slightly\n",
    "for key in features2:\n",
    "    features2[key] = features2[key] + np.random.randn(len(features2[key])) * 0.1\n",
    "\n",
    "concat_features2 = taxonomy.get_concatenated_features(features2)\n",
    "hypervector2 = encoder.encode_vector(concat_features2)\n",
    "\n",
    "# Compare using Hamming distance\n",
    "from src.hypervector.hamming import HammingDistanceOptimized\n",
    "\n",
    "hamming = HammingDistanceOptimized()\n",
    "distance = hamming.distance(hypervector, hypervector2)\n",
    "similarity = 1.0 - (distance / len(hypervector))\n",
    "\n",
    "print(\"üìä Hypervector Comparison:\")\n",
    "print(f\"  Hamming distance: {distance:,} / {len(hypervector):,}\")\n",
    "print(f\"  Similarity: {similarity:.2%}\")\n",
    "\n",
    "# Visualize differences\n",
    "diff = hypervector.astype(int) - hypervector2.astype(int)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(diff[:2000].reshape(40, 50), cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Difference (-1: removed, 0: same, 1: added)')\n",
    "plt.title('Hypervector Differences (First 2000 bits)')\n",
    "plt.xlabel('Bit Index')\n",
    "plt.ylabel('Row')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Orchestration <a id='orchestration'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt orchestrator\n",
    "orchestrator = PromptOrchestrator()\n",
    "\n",
    "# Test different strategies\n",
    "strategies = ['balanced', 'adversarial', 'behavioral', 'comprehensive']\n",
    "strategy_prompts = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    prompts = orchestrator.generate_prompts(n=10, strategy=strategy)\n",
    "    strategy_prompts[strategy] = prompts\n",
    "    \n",
    "    print(f\"\\nüéØ Strategy: {strategy.upper()}\")\n",
    "    print(f\"Generated {len(prompts)} prompts\")\n",
    "    print(\"\\nSample prompts:\")\n",
    "    for i, prompt in enumerate(prompts[:2], 1):\n",
    "        display = prompt[:100] + \"...\" if len(prompt) > 100 else prompt\n",
    "        print(f\"  {i}. {display}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prompt characteristics\n",
    "def analyze_prompts(prompts):\n",
    "    \"\"\"Analyze prompt characteristics.\"\"\"\n",
    "    lengths = [len(p) for p in prompts]\n",
    "    return {\n",
    "        'count': len(prompts),\n",
    "        'avg_length': np.mean(lengths),\n",
    "        'std_length': np.std(lengths),\n",
    "        'min_length': np.min(lengths),\n",
    "        'max_length': np.max(lengths)\n",
    "    }\n",
    "\n",
    "# Compare strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Length distributions\n",
    "ax = axes[0]\n",
    "for strategy in strategies:\n",
    "    lengths = [len(p) for p in strategy_prompts[strategy]]\n",
    "    ax.hist(lengths, alpha=0.5, label=strategy, bins=10)\n",
    "\n",
    "ax.set_xlabel('Prompt Length (characters)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Prompt Length Distributions by Strategy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary statistics\n",
    "ax = axes[1]\n",
    "stats_data = []\n",
    "for strategy in strategies:\n",
    "    stats = analyze_prompts(strategy_prompts[strategy])\n",
    "    stats_data.append(stats['avg_length'])\n",
    "\n",
    "ax.bar(strategies, stats_data, color=['C0', 'C1', 'C2', 'C3'])\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Average Prompt Length')\n",
    "ax.set_title('Average Prompt Length by Strategy')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nüìä Prompt Statistics by Strategy:\")\n",
    "for strategy in strategies:\n",
    "    stats = analyze_prompts(strategy_prompts[strategy])\n",
    "    print(f\"\\n{strategy.capitalize()}:\")\n",
    "    for key, value in stats.items():\n",
    "        if 'length' in key:\n",
    "            print(f\"  {key}: {value:.1f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory-Bounded Execution <a id='memory'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get system memory info\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "print(\"üíæ System Memory:\")\n",
    "print(f\"  Total: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"  Available: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"  Used: {memory.percent:.1f}%\")\n",
    "\n",
    "# Memory configurations for different scenarios\n",
    "configs = [\n",
    "    {\"name\": \"Minimal\", \"limit_gb\": 1.0, \"use_case\": \"<16GB RAM\"},\n",
    "    {\"name\": \"Conservative\", \"limit_gb\": 2.0, \"use_case\": \"16-32GB RAM\"},\n",
    "    {\"name\": \"Balanced\", \"limit_gb\": 4.0, \"use_case\": \"32-64GB RAM\"},\n",
    "    {\"name\": \"Performance\", \"limit_gb\": 8.0, \"use_case\": \"64GB+ RAM\"}\n",
    "]\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Recommended Configurations:\")\n",
    "for config in configs:\n",
    "    feasible = \"‚úÖ\" if config[\"limit_gb\"] <= memory.available / (1024**3) else \"‚ùå\"\n",
    "    print(f\"  {feasible} {config['name']}: {config['limit_gb']} GB ({config['use_case']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate memory usage patterns\n",
    "time_points = np.arange(0, 100, 1)\n",
    "memory_patterns = {\n",
    "    'No limit': 8 + 2 * np.sin(time_points/10) + np.random.randn(100) * 0.5,\n",
    "    '4GB limit': np.minimum(4, 3 + np.sin(time_points/10) + np.random.randn(100) * 0.3),\n",
    "    '2GB limit': np.minimum(2, 1.5 + 0.5 * np.sin(time_points/10) + np.random.randn(100) * 0.2),\n",
    "    '1GB limit': np.minimum(1, 0.8 + 0.2 * np.sin(time_points/10) + np.random.randn(100) * 0.1)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for pattern, values in memory_patterns.items():\n",
    "    plt.plot(time_points, values, label=pattern, linewidth=2)\n",
    "\n",
    "plt.axhline(y=4, color='red', linestyle='--', alpha=0.5, label='4GB threshold')\n",
    "plt.axhline(y=2, color='orange', linestyle='--', alpha=0.5, label='2GB threshold')\n",
    "plt.axhline(y=1, color='yellow', linestyle='--', alpha=0.5, label='1GB threshold')\n",
    "\n",
    "plt.xlabel('Time (arbitrary units)')\n",
    "plt.ylabel('Memory Usage (GB)')\n",
    "plt.title('Memory Usage Patterns with Different Limits')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"  ‚Ä¢ Without limits: Memory usage can spike unpredictably\")\n",
    "print(\"  ‚Ä¢ With limits: Memory stays bounded, preventing OOM errors\")\n",
    "print(\"  ‚Ä¢ Trade-off: Lower limits = slower processing but more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Testing (SPRT) <a id='statistics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPRT\n",
    "sprt = SequentialTest(\n",
    "    alpha=0.05,  # Type I error\n",
    "    beta=0.05,   # Type II error\n",
    "    theta_0=0.5, # Null hypothesis\n",
    "    theta_1=0.7  # Alternative hypothesis\n",
    ")\n",
    "\n",
    "print(\"üìä SPRT Configuration:\")\n",
    "print(f\"  Œ± (Type I error): {sprt.alpha}\")\n",
    "print(f\"  Œ≤ (Type II error): {sprt.beta}\")\n",
    "print(f\"  H‚ÇÄ: Œ∏ = {sprt.theta_0}\")\n",
    "print(f\"  H‚ÇÅ: Œ∏ = {sprt.theta_1}\")\n",
    "print(f\"  Upper boundary: {sprt.upper_threshold:.3f}\")\n",
    "print(f\"  Lower boundary: {sprt.lower_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate SPRT with different scenarios\n",
    "from src.core.sequential import TestDecision\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"H‚ÇÄ True\", \"p\": 0.5},\n",
    "    {\"name\": \"H‚ÇÅ True\", \"p\": 0.7},\n",
    "    {\"name\": \"Borderline\", \"p\": 0.6}\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, scenario in zip(axes, scenarios):\n",
    "    # Reset test\n",
    "    test = SequentialTest(alpha=0.05, beta=0.05, theta_0=0.5, theta_1=0.7)\n",
    "    \n",
    "    # Generate samples\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    samples = np.random.binomial(1, scenario['p'], 100)\n",
    "    \n",
    "    # Run test\n",
    "    likelihood_ratios = [1.0]\n",
    "    decisions = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        decision = test.add_sample(sample)\n",
    "        likelihood_ratios.append(test.likelihood_ratio)\n",
    "        decisions.append(decision)\n",
    "        \n",
    "        if decision != TestDecision.CONTINUE:\n",
    "            break\n",
    "    \n",
    "    # Plot\n",
    "    x = range(len(likelihood_ratios))\n",
    "    ax.plot(x, likelihood_ratios, 'b-', linewidth=2)\n",
    "    ax.axhline(y=test.upper_threshold, color='green', linestyle='--', label='Accept H‚ÇÅ')\n",
    "    ax.axhline(y=test.lower_threshold, color='red', linestyle='--', label='Accept H‚ÇÄ')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Sample Number')\n",
    "    ax.set_ylabel('Likelihood Ratio (log scale)')\n",
    "    ax.set_title(f'{scenario[\"name\"]} (p={scenario[\"p\"]})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add decision point\n",
    "    if decision != TestDecision.CONTINUE:\n",
    "        ax.plot(len(likelihood_ratios)-1, likelihood_ratios[-1], 'ro', markersize=10)\n",
    "        ax.text(len(likelihood_ratios)-1, likelihood_ratios[-1], \n",
    "                f'  Decision: {decision.value}\\n  Samples: {len(likelihood_ratios)-1}',\n",
    "                fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('SPRT Performance in Different Scenarios', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Key Insights:\")\n",
    "print(\"  ‚Ä¢ SPRT reaches decisions with fewer samples than fixed tests\")\n",
    "print(\"  ‚Ä¢ Clear cases (H‚ÇÄ or H‚ÇÅ true) decide quickly\")\n",
    "print(\"  ‚Ä¢ Borderline cases take more samples but maintain error bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analysis <a id='advanced'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline demonstration\n",
    "print(\"üöÄ Running Complete REV Pipeline\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Initialize with all features\n",
    "print(\"\\n1Ô∏è‚É£ Initializing REV with all features...\")\n",
    "rev_advanced = REVUnified(\n",
    "    memory_limit_gb=2.0,\n",
    "    enable_prompt_orchestration=True,\n",
    "    enable_principled_features=True,\n",
    "    unified_fingerprints=True,\n",
    "    fingerprint_dimension=10000,\n",
    "    debug=False\n",
    ")\n",
    "print(\"   ‚úÖ Initialized\")\n",
    "\n",
    "# Step 2: Process model\n",
    "print(\"\\n2Ô∏è‚É£ Processing model with advanced features...\")\n",
    "if 'MODEL_PATH' in globals() and Path(MODEL_PATH).exists():\n",
    "    advanced_result = rev_advanced.process_model(\n",
    "        MODEL_PATH,\n",
    "        challenges=10\n",
    "    )\n",
    "    print(\"   ‚úÖ Processing complete\")\n",
    "    \n",
    "    # Step 3: Display comprehensive results\n",
    "    print(\"\\n3Ô∏è‚É£ Analysis Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\nüìä Model Information:\")\n",
    "    print(f\"  Family: {advanced_result.get('model_family', 'Unknown')}\")\n",
    "    print(f\"  Architecture: {advanced_result.get('architecture', 'Unknown')}\")\n",
    "    print(f\"  Confidence: {advanced_result.get('confidence', 0):.2%}\")\n",
    "    \n",
    "    if 'restriction_sites' in advanced_result:\n",
    "        print(f\"\\nüß¨ Restriction Sites:\")\n",
    "        sites = advanced_result['restriction_sites']\n",
    "        print(f\"  Total sites: {len(sites)}\")\n",
    "        if sites:\n",
    "            print(f\"  Highest divergence: Layer {sites[0]['layer_idx']} \"\n",
    "                  f\"({sites[0]['behavioral_divergence']:.3f})\")\n",
    "    \n",
    "    if 'fingerprint' in advanced_result:\n",
    "        print(f\"\\nüîê Fingerprint:\")\n",
    "        fp = advanced_result['fingerprint']\n",
    "        print(f\"  Dimension: {fp.get('dimension', 'Unknown')}\")\n",
    "        print(f\"  Sparsity: {fp.get('sparsity', 0):.2%}\")\n",
    "        print(f\"  Pathways: {', '.join(fp.get('pathways', []))}\")\n",
    "    \n",
    "    if 'metrics' in advanced_result:\n",
    "        print(f\"\\n‚ö° Performance:\")\n",
    "        metrics = advanced_result['metrics']\n",
    "        print(f\"  Processing time: {metrics.get('time', 0):.1f}s\")\n",
    "        print(f\"  Memory peak: {metrics.get('memory_gb', 0):.1f}GB\")\n",
    "        print(f\"  Samples used: {metrics.get('samples', 0)}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    rev_advanced.cleanup()\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Model not found. Please set MODEL_PATH to a valid model.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Pipeline demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Basic model verification\n",
    "- ‚úÖ 56 principled features extraction\n",
    "- ‚úÖ Hyperdimensional fingerprint generation\n",
    "- ‚úÖ Prompt orchestration strategies\n",
    "- ‚úÖ Memory-bounded execution\n",
    "- ‚úÖ Statistical testing with SPRT\n",
    "- ‚úÖ Complete pipeline integration\n",
    "\n",
    "### Next Steps:\n",
    "1. Try with your own models\n",
    "2. Experiment with different parameters\n",
    "3. Build reference library for model families\n",
    "4. Deploy production API\n",
    "5. Set up monitoring dashboards\n",
    "\n",
    "### Resources:\n",
    "- [GitHub Repository](https://github.com/rohanvinaik/REV)\n",
    "- [Documentation](../docs/)\n",
    "- [API Reference](../docs/API_REFERENCE.md)\n",
    "- [User Guide](../docs/USER_GUIDE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}