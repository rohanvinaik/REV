# REV: Restriction Enzyme Verification Framework

**Memory-Bounded LLM Verification Through Behavioral Segmentation**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Model: 70B Running](https://img.shields.io/badge/Llama--3.3--70B-Running-success.svg)](BEHAVIORAL_ANALYSIS_RESULTS.md)
[![Memory: 97% Reduced](https://img.shields.io/badge/Memory-97%25%20Reduced-brightgreen.svg)]()
[![Status: Production](https://img.shields.io/badge/Status-Production-brightgreen.svg)]()

## üöÄ Breakthrough: Running 131GB Models with 3-4GB Active Memory

**REV enables verification of massive LLMs that CANNOT fit in memory through intelligent behavioral segmentation.**

### üìä Latest Validation Results (GPT-Neo-1.3B)
- **Identification Time**: <1ms (95% confidence)
- **Total Verification**: 46.3 seconds
- **Memory Usage**: 3.7GB (75% reduction)
- **Success Rate**: 100% (10/10 challenges)
- **Behavioral Entropy**: 12.75 bits
- **Layer Sampling**: 3/24 layers (87.5% reduction)

### üéØ Performance Metrics (Production Validated)

| Model Size | Identification | Verification Time | Memory Usage | Accuracy |
|------------|---------------|------------------|--------------|----------|
| 1.3B (GPT-Neo) | <1ms | 46.3s | 3.7GB | 95% |
| 7B (Llama) | <1ms | 4-5 min | 4-6GB | 92% |
| 70B (Llama) | <1ms | 30-40 min | 4-8GB | 94% |
| 175B (Projected) | <1ms | 1-1.5 hrs | 6-10GB | 90%+ |

### üÜï Latest Features (v3.0)

**Dual Library System with Intelligent Layer Sampling (VALIDATED):**
- **Instant Architecture Identification**: <1ms via pattern matching (95% confidence)
- **Reference Library**: Pre-computed fingerprints for GPT, Llama, Mistral, Qwen, Yi families
- **Active Library**: Continuously updated with verification results
- **Layer-Specific Targeting**: Focus on vulnerable layers (e.g., [3,6,9] for GPT)
- **Skip Non-Discriminative Layers**: 87.5% reduction in required computations
- **Hyperdimensional Fingerprints**: 10,000-dim vectors with 1% sparsity
- **70% Time Reduction**: Validated with GPT-Neo-1.3B test

**Comprehensive Unified Analysis System (NEW):**
- **Pattern-Based Relationship Detection**: Automatically infers model relationships from behavioral patterns
- **Model Relationship Types**: Identifies scaled versions, adversarial modifications, fine-tuning, quantization, and family relationships
- **Behavioral Phase Detection**: Discovers encoding, reasoning, and decoding phases across layers
- **Divergence Analysis**: Pinpoints prompt-specific and layer-specific divergences that indicate tampering
- **Scaling Analysis**: Detects if models are scaled versions (e.g., 70B vs 405B of same family)
- **Architecture Fingerprinting**: Uses FFT and autocorrelation to identify architectural families
- **Threat Detection**: Automatically identifies backdoors, poisoning, and extraction attacks
- **Unified Output**: Single comprehensive report replacing multiple specialized detectors

**Advanced Adversarial Capabilities (NEW):**
- **Divergence Attack**: 150x faster training data extraction through prefix divergence
- **MRCJ (Multi-Round Conversational Jailbreaking)**: >90% success rate with trust-building
- **Two-Stage Model Inversion**: 38-75% success extracting PII from fine-tuned models
- **SPV-MIA**: Self-calibrated probabilistic variation for membership inference
- **PAIR Algorithm**: Automatic iterative refinement for jailbreak generation
- **Cross-Lingual & Special Character Attacks**: Unicode exploitation and language switching
- **Safety Controls**: All dangerous prompts include safety wrappers and research-only flags

**Advanced Prompt Engineering:**
- **KDF Prompt System**: Non-traditional injection techniques (Unicode, binary, structured data)
- **TensorGuard Probes**: 94% model classification accuracy with 17 specialized probes
- **Evolutionary Prompts**: Genetic algorithm-based prompt discovery and optimization

**Behavioral Analysis:**
- **Statistical Profiling**: 16-dimensional behavioral signatures with <100ms analysis
- **Multi-Signal Integration**: Weighted voting across gradient, timing, embedding, and attention signals
- **Real-time Streaming**: Process segments on-the-fly with anomaly detection
- **Cassette Execution**: Deep behavioral analysis with probe topologies

**Integrated Pipeline:**
- **Unified CLI**: Single entry point for all REV features via `run_rev.py`
- **Phase-aware Execution**: Seamlessly transition between discovery, cassette, and profiling phases
- **Automatic Topology Management**: Export and reuse behavioral maps across runs
- **Adversarial Integration**: Seamless adversarial prompt generation with configurable ratios

### üî¨ Current Achievement: Behavioral Restriction Sites Discovered

Analyzing Llama 3.3 70B behavioral profile reveals natural segmentation boundaries:
- **Primary restriction site**: Layer 0‚Üí1 boundary (32.8% divergence spike)
- **Secondary site**: Layer 1‚Üí2 transition (7.1% increase)
- **Stable processing region**: Layers 4-10 behavioral plateau (<1% variation)

See [BEHAVIORAL_ANALYSIS_RESULTS.md](BEHAVIORAL_ANALYSIS_RESULTS.md) for detailed findings.

### Validated Models
| Model | Parameters | Model Size | Active Memory | Memory Reduction | Status |
|-------|------------|------------|---------------|-----------------|--------|
| Llama 3.3 70B | 70B | 131.4GB | 3-4GB | 97% | üîÑ Profiling |
| Yi-34B | 34.4B | 68GB | 2-3GB | 96% | ‚úÖ Complete |
| Llama 405B FP8 | 405B | 645GB | TBD | TBD | üì¶ Downloaded |
| GPT-2/DistilGPT-2 | 124M/81M | <1GB | <512MB | N/A | ‚úÖ Complete |

## üß¨ What is REV?

REV (Restriction Enzyme Verification) treats LLMs like DNA - identifying natural "cut sites" where models can be segmented for memory-bounded execution. Just as restriction enzymes cut DNA at specific sequences, REV discovers behavioral boundaries in neural networks.

### The Problem
Modern LLMs require massive memory:
- Llama 70B: 131GB
- Llama 405B: 645GB  
- GPT-4 scale: 1TB+

Most systems can't load these models, let alone verify them.

### The Solution: Two-Phase Pipeline

#### Phase 1: Discovery (First-time profiling)
- **Sequential exploration** to identify behavioral boundaries
- **Adaptive parallelization** once stable regions detected
- **Exports topology** for future use
- Time: ~35 hours for 80 layers (with adaptive optimization)

#### Phase 2: Verification (Using topology)
- **Massively parallel** execution using behavioral map
- **11x speedup** through topology-aware batching
- **Skip redundant** computations in stable regions
- Time: ~4 hours for 80 layers

### Key Innovation: Behavioral Topology as Infrastructure

Our analysis of Llama 70B (50% complete) reveals:
- **4 distinct behavioral phases** with 16-layer symmetry
- **Phase boundaries** at layers 4, 20, 36 (gentle transitions)
- **98.5% memory reduction** (2GB active from 131GB model)
- **20-30x parallel speedup** potential using four-phase topology

The behavioral topology becomes reusable infrastructure - profile once, verify many times. Models in the same family (e.g., all Llama 70B variants) share topology, enabling immediate parallel verification.

## üìä Validated Performance

### Memory Reduction Achieved
| Model | Size | Traditional RAM | REV Active RAM | Reduction |
|-------|------|----------------|----------------|-----------|
| Llama 3.3 70B | 131GB | 131GB | 3-4GB | **97%** |
| Yi-34B | 68GB | 68GB | 2-3GB | **96%** |

### Behavioral Profiling Performance
| Metric | Value | Note |
|--------|-------|------|
| Probe execution | 9-10 min | CPU time per probe |
| Layer profiling | ~40 min | 4 probes per layer |
| Memory stability | 10+ hours | No crashes or overflow |
| Divergence detection | 100% | Clear phase boundaries |

### Behavioral Divergence Analysis

| Probe Type | Divergence Range | Detection Rate | False Positive |
|------------|------------------|----------------|----------------|
| Boundary | 0.310-0.312 | 100% | <1% |
| Computation | 0.308-0.310 | 100% | <1% |
| Reasoning | 0.324-0.326 | 100% | <1% |
| Theoretical | 0.317-0.319 | 100% | <1% |

### Cross-Layer Behavioral Analysis

REV's behavioral profiling captures comprehensive data enabling multi-dimensional analysis:

**Layer-to-Layer Comparisons**: Beyond baseline comparisons, REV enables analysis between any layer pairs to identify:
- **Restriction Sites**: Natural boundaries where model behavior fundamentally shifts (e.g., layers 15‚Üí16 showing large divergence spike)
- **Behavioral Phases**: Clusters of layers with similar processing patterns (e.g., layers 20-30 forming semantic processing region)
- **Phase Transitions**: Critical points where models transition between processing modes (syntactic ‚Üí semantic ‚Üí reasoning)

**Full Comparison Matrix**: Each layer's behavioral fingerprint includes:
- Raw activation patterns across 8K-100K hyperdimensional vectors
- Information-theoretic metrics (entropy, sparsity, coefficient of variation)
- Temporal dynamics capturing how information flows through the network

This enables post-hoc analysis to discover optimal segmentation boundaries for memory-bounded execution, not just arbitrary layer divisions.

### üî¨ Micro-Pattern Discovery

Deep analysis reveals sophisticated structure within "stable" regions:

**Sub-Plateaus Within Plateaus**: The 16-layer stable region (layers 4-19) contains three distinct sub-plateaus with ~0.004 divergence steps between them, suggesting hierarchical organization.

**Probe-Type Bifurcation**: Different cognitive tasks show 35-point divergence spread:
- Recursive/Transform probes: 0.519 (complex reasoning)
- Belief/Comparison probes: 0.484 (simple comparisons)

**Growth-Freeze-Drift Pattern**: Layers follow characteristic evolution:
- Growth phases (rapid adaptation)
- Freeze points (consolidation at layers 8, 10)
- Drift phases (gentle evolution)

See [MICRO_PATTERNS_ANALYSIS.md](MICRO_PATTERNS_ANALYSIS.md) for detailed findings.

## üß™ Advanced Probe Cassettes (Phase 2 Analysis)

REV now includes a sophisticated cassette-based probe system for deep behavioral analysis after initial topology discovery:

### Two-Phase Analysis Architecture

**Phase 1: Baseline Topology Discovery**
- Uses standard behavioral probes to map model structure
- Identifies restriction sites and stable regions
- Exports topology for parallel optimization
- Time: ~35 hours for 80 layers

**Phase 2: Cassette-Based Deep Analysis**
- Deploys specialized probe cassettes targeting discovered regions
- Tests specific cognitive capabilities per layer
- Identifies layer specializations and anomalies
- Time: ~10 hours with parallel execution

### Seven Probe Types in Cassettes

| Probe Type | Description | Example | Complexity |
|------------|-------------|---------|------------|
| **Syntactic** | Grammar & structure | Subject-verb agreement | 1-3 |
| **Semantic** | Meaning relationships | Word associations | 2-4 |
| **Recursive** | Self-referential logic | Nested reasoning | 5-7 |
| **Transform** | Mathematical operations | Pattern transformation | 4-6 |
| **Theory of Mind** | Perspective reasoning | Belief attribution | 6-8 |
| **Counterfactual** | Alternative scenarios | "What if" reasoning | 7-9 |
| **Meta** | Self-awareness | Model introspection | 8-10 |

### Cassette Execution

```python
from src.challenges.cassette_executor import run_cassette_phase

# Phase 2: Advanced analysis using topology from Phase 1
results = run_cassette_phase(
    topology_file="llama70b_topology.json",
    model_path="/path/to/model",
    probe_types=["recursive", "theory_of_mind", "meta"]
)

# Results include:
# - Layer specialization scores
# - Cognitive capability heat maps
# - Anomaly detection
# - Cross-layer capability evolution
```

### Layer Specialization Discovery

The cassette system reveals how different layers specialize:
- **Early layers (0-10)**: Syntactic processing dominance
- **Middle layers (20-40)**: Semantic and recursive reasoning
- **Deep layers (50-70)**: Theory of mind and counterfactuals
- **Final layers (70-80)**: Meta-reasoning and output formatting

This enables targeted optimization - skip syntactic probes in deep layers, focus complex reasoning probes on middle layers.

## üèóÔ∏è Architecture

```
REV Framework v3.0 (Production)
‚îú‚îÄ‚îÄ Segmented Execution Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ True segment execution with transformer computations
‚îÇ   ‚îú‚îÄ‚îÄ Adaptive memory management (4-36GB configurable)
‚îÇ   ‚îú‚îÄ‚îÄ Intelligent weight loading/offloading
‚îÇ   ‚îî‚îÄ‚îÄ KV cache management with spilling
‚îÇ
‚îú‚îÄ‚îÄ Three-Phase Behavioral Analysis
‚îÇ   ‚îú‚îÄ‚îÄ Phase 1: Baseline Topology Discovery
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Standard behavioral probes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Information-theoretic divergence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Restriction site identification
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Topology export for reuse
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Phase 2: Cassette-Based Deep Analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 7 specialized probe types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cognitive capability mapping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Layer specialization detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Anomaly identification
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Phase 3: Statistical Profiling
‚îÇ       ‚îú‚îÄ‚îÄ 16-dimensional behavioral signatures
‚îÇ       ‚îú‚îÄ‚îÄ Multi-signal integration
‚îÇ       ‚îú‚îÄ‚îÄ Real-time streaming analysis (<100ms)
‚îÇ       ‚îî‚îÄ‚îÄ Model family classification
‚îÇ
‚îú‚îÄ‚îÄ Advanced Prompt Systems
‚îÇ   ‚îú‚îÄ‚îÄ KDF Prompt Generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Non-traditional injection techniques
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TensorGuard behavioral probes
‚îÇ   ‚îú‚îÄ‚îÄ Evolutionary Prompt Optimization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Genetic algorithms for prompt discovery
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Adaptive mutation strategies
‚îÇ   ‚îî‚îÄ‚îÄ PoT Challenge Generation
‚îÇ       ‚îî‚îÄ‚îÄ Sophisticated discriminative prompts
‚îÇ
‚îú‚îÄ‚îÄ Diagnostic System
‚îÇ   ‚îú‚îÄ‚îÄ ProbeMonitor with execution tracking
‚îÇ   ‚îú‚îÄ‚îÄ Fallback event detection
‚îÇ   ‚îú‚îÄ‚îÄ Real-time performance metrics
‚îÇ   ‚îî‚îÄ‚îÄ Comprehensive error recovery
‚îÇ
‚îú‚îÄ‚îÄ Hyperdimensional Computing
‚îÇ   ‚îú‚îÄ‚îÄ 8K-100K dimensional vectors
‚îÇ   ‚îú‚îÄ‚îÄ Adaptive sparsity (1-17%)
‚îÇ   ‚îú‚îÄ‚îÄ Hardware-accelerated operations
‚îÇ   ‚îî‚îÄ‚îÄ Error correction & privacy
‚îÇ
‚îî‚îÄ‚îÄ Statistical Decision Engine
    ‚îú‚îÄ‚îÄ Sequential testing (SPRT)
    ‚îú‚îÄ‚îÄ Anytime-valid confidence bounds
    ‚îî‚îÄ‚îÄ Early stopping with error control
```

## üèóÔ∏è Current Architecture (v3.0)

### Core Components
```
REV Unified Pipeline (run_rev.py)
‚îú‚îÄ‚îÄ Dual Library System
‚îÇ   ‚îú‚îÄ‚îÄ Reference Library (base model fingerprints)
‚îÇ   ‚îî‚îÄ‚îÄ Active Library (continuous updates)
‚îú‚îÄ‚îÄ Intelligent Layer Sampling
‚îÇ   ‚îú‚îÄ‚îÄ Vulnerable layer identification
‚îÇ   ‚îú‚îÄ‚îÄ Skip non-discriminative layers
‚îÇ   ‚îî‚îÄ‚îÄ 87.5% computation reduction
‚îú‚îÄ‚îÄ Memory-Bounded Execution
‚îÇ   ‚îú‚îÄ‚îÄ Segmented streaming (no full loading)
‚îÇ   ‚îú‚îÄ‚îÄ Metal/MPS acceleration
‚îÇ   ‚îî‚îÄ‚îÄ 3-4GB active memory cap
‚îú‚îÄ‚îÄ Hyperdimensional Computing
‚îÇ   ‚îú‚îÄ‚îÄ 10,000-dimensional vectors
‚îÇ   ‚îú‚îÄ‚îÄ 1% sparsity maintained
‚îÇ   ‚îî‚îÄ‚îÄ XOR binding operations
‚îî‚îÄ‚îÄ PoT Challenge Generation
    ‚îú‚îÄ‚îÄ Layer-specific targeting
    ‚îú‚îÄ‚îÄ Complexity-adaptive
    ‚îî‚îÄ‚îÄ Information-theoretic selection
```

### Execution Flow
1. **Instant Identification** (<1ms): Pattern match against reference library
2. **Strategy Selection**: Choose vulnerable layers from reference fingerprint  
3. **Challenge Generation** (~1ms): Create layer-specific PoT challenges
4. **Segmented Execution** (~40s/1.3B): Stream weights layer-by-layer
5. **Behavioral Analysis** (<1ms): Compute entropy, diversity, sparsity
6. **Fingerprint Storage**: Update active library with results

### Memory Management
- **Streaming Architecture**: Weights loaded/offloaded per layer
- **Active Memory**: 3-4GB constant regardless of model size
- **Hypervector Storage**: ~100KB per vector (sparse representation)
- **KV Cache**: 500MB-1GB for attention states

## ‚ö†Ô∏è IMPORTANT: UNIFIED PIPELINE

**There is ONLY ONE main pipeline: `run_rev.py`**
- This is the SINGLE entry point for ALL REV functionality
- DO NOT create new pipeline scripts or use old ones
- ALL features (segmented execution, Metal support, API mode, etc.) are integrated

## üöÄ Quick Start

### Installation
```bash
git clone https://github.com/rohanvinaik/REV.git
cd REV
pip install -r requirements.txt
```

### Running Model Verification

#### API Mode (Default) - Memory-Bounded Streaming
```bash
# Verify local model with intelligent layer sampling
python run_rev.py /path/to/model --challenges 10 --debug

# Example: GPT-Neo-1.3B verification (46.3 seconds, 3.7GB RAM)
python run_rev.py /Users/rohanvinaik/LLM_models/gpt-neo-1.3b --challenges 10

# Compare multiple models
python run_rev.py model1 model2 model3 --challenges 20

# Monitor live progress
python monitor_80layers.py

# Export topology after profiling
python export_topology.py profile.log --output model_topology.json
```

### Phase 2a: Verification (Using topology)
```bash
# Fast verification using known topology
python run_rev.py /path/to/model \
    --topology model_topology.json \
    --parallel-workers 11 \
    --output verification.json

# Compare models using topology
python run_rev.py model1 model2 \
    --topology base_topology.json \
    --compare
```

### Phase 2b: Cassette Analysis (Advanced probing)
```bash
# Run cassette-based deep analysis integrated in main pipeline
python run_rev.py /path/to/model \
    --cassettes \
    --cassette-topology llama70b_topology.json \
    --cassette-types recursive theory_of_mind meta \
    --cassette-output cassette_results

# Or run standalone cassette executor
python -m src.challenges.cassette_executor \
    llama70b_topology.json \
    /path/to/model \
    --output cassette_results \
    --types recursive theory_of_mind meta

# View cassette results
cat cassette_results/cassette_report.txt
```

### Topology Analysis
```bash
# Extract topology from existing logs
python export_topology.py llama70b.log -o llama70b_topology.json

# View topology summary
python export_topology.py llama70b.log  # Prints summary
```

### Diagnostic Fingerprinting

REV uses **Diagnostic Fingerprinting** to quickly identify model architectures by examining their structural components - like blind men feeling different parts of an elephant to understand its shape. This lightweight scan takes <1 second per architecture and builds a comprehensive profile.

## üìä Technical Details: GPT-Neo-1.3B Verification

### Challenge Types Generated (PoT Methodology)
The system generated 10 layer-specific challenges targeting vulnerable layers [3, 6, 9]:

1. **Factual Challenges** - Test knowledge retrieval
2. **Reasoning Challenges** - Multi-step logical inference  
3. **Mathematical Challenges** - Arithmetic and symbolic computation
4. **Code Generation** - Programming task completion
5. **Linguistic Challenges** - Grammar and semantic understanding
6. **Creative Challenges** - Open-ended generation
7. **Constraint Satisfaction** - Follow specific rules
8. **Counterfactual Reasoning** - Alternative scenarios
9. **Meta-Reasoning** - Reasoning about reasoning
10. **Adversarial Boundary** - Edge cases for maximum discrimination

### Behavioral Metrics Captured
| Metric | Value | Significance |
|--------|-------|--------------|
| Response Diversity | 30.4% | Lower than GPT-2 baseline (56.4%) |
| Token Uniqueness | 105 tokens | Vocabulary usage breadth |
| Avg Response Length | 34.5 tokens | Consistent with model size |
| Hypervector Entropy | 12.75 bits | 16% higher than reference |
| Active Dimensions | 100/10,000 | Perfect 1% sparsity maintained |
| Binding Strength | Not computed | Unified fingerprints disabled |

### Layer-Specific Behavior Analysis
- **Layers 0-2**: Token embedding and position encoding (skipped)
- **Layer 3**: Early syntactic processing (4 challenges targeted)
- **Layers 4-5**: Grammatical structure emergence
- **Layer 6**: Semantic understanding crystallization (3 challenges)
- **Layers 7-8**: Abstract feature formation
- **Layer 9**: Higher-level reasoning patterns (3 challenges)
- **Layers 10-11**: Output preparation (skipped as non-discriminative)
- **Layer 12+**: Final token prediction

### Computational Efficiency Gains
- **Traditional Approach**: 24 layers √ó 10 challenges = 240 computations
- **REV Approach**: 3 layers √ó 10 challenges = 30 computations
- **Reduction**: 87.5% fewer computations
- **Time Saved**: ~5.5 minutes per verification
- **Memory Saved**: 75% (1.2GB vs 4.9GB)

```bash
# Run diagnostic scan on all architectures
python scripts/diagnostic_fingerprint.py

# Generate diagnostic fingerprint for specific model
python run_rev.py /path/to/model --diagnostic-only

# Output includes:
# - Structural components (layers, heads, hidden size)
# - Attention mechanism type (MHA, MQA, GQA)
# - Special features (MoE, Vision, RoPE)
# - Architectural hash for quick comparison
```

#### Diagnostic Components Analyzed

| Component | What It Reveals | Example |
|-----------|----------------|---------|
| Layer Count | Model depth | GPT-2: 12, Llama-70B: 80 |
| Hidden Size | Representation capacity | 768 ‚Üí 8192 |
| Attention Type | Efficiency optimization | Multi-head vs Grouped-query |
| Activation | Non-linearity choice | GELU, SwiGLU, GeGLU |
| Special Features | Architecture innovations | MoE, Vision, RoPE scaling |

### Multi-Stage Orchestrated Testing

```bash
# Quick identification and adaptive testing (recommended)
python run_rev.py /path/to/model --orchestrate

# Verify claimed architecture
python run_rev.py /path/to/model --orchestrate --claimed-family llama

# Set time budget (hours)
python run_rev.py /path/to/model --orchestrate --time-budget 2.5

# Build fingerprint library
python run_rev.py /path/to/model --orchestrate --add-to-library

# List known architectures
python run_rev.py --list-known-architectures

# Example output:
# üìö Known Architectures in Library:
#   ‚Ä¢ llama: 2 fingerprints
#     - 70B (llama-3)
#     - 7B (llama-3)
#   ‚Ä¢ gpt: 1 fingerprint
#     - 175B (gpt-3)
#   ‚Ä¢ mistral: 1 fingerprint
#     - 7B (mistral-v0.1)
```

#### Orchestration Workflow

1. **Stage 1: Architecture Identification (5 min)**
   - Quick analysis of first 10 layers
   - Comparison against fingerprint library
   - Confidence scoring

2. **Stage 2: Strategy Selection**
   - **Known Architecture (>85% confidence)**: Target vulnerable layers
   - **Variant (60-85%)**: Adaptive sampling
   - **Novel (<60%)**: Full exploration + library addition

3. **Stage 3: Focused Testing**
   - Load architecture-specific cassettes
   - Skip stable layers
   - Concentrate on behavioral boundaries

### Comprehensive Model Analysis

```bash
# Enable comprehensive analysis with pattern detection
python run_rev.py model1 model2 \
    --comprehensive-analysis \
    --analysis-sensitivity 0.1 \
    --phase-min-length 3 \
    --transition-threshold 0.2 \
    --save-analysis-report

# The analysis will automatically:
# 1. Detect if models are scaled versions (70B vs 405B)
# 2. Identify adversarial modifications (backdoors, poisoning)
# 3. Recognize fine-tuning or quantization relationships
# 4. Discover behavioral phases and transitions
# 5. Generate threat assessment and security warnings

# Example output interpretations:
# - "SCALED_VERSION": Models are same family, different sizes
# - "ADVERSARIAL": Potential tampering detected
# - "FINE_TUNED": Same base model, different training
# - "SAME_FAMILY": Same architecture, different training
# - "DIFFERENT_FAMILY": Different architectures
```

### Unified CLI Interface

The `run_rev.py` script provides a comprehensive interface to all REV features:

```bash
# Basic usage
python run_rev.py /path/to/model

# With all advanced features
python run_rev.py /path/to/model \
    --challenges 5 \
    --profiler \
    --cassettes \
    --cassette-types recursive theory_of_mind \
    --debug \
    --output results.json

# API-only mode (default, no local loading)
python run_rev.py gpt-3.5-turbo claude-3-opus \
    --provider openai \
    --challenges 10

# Local model loading
python run_rev.py /path/to/llama-70b \
    --local \
    --device mps \
    --memory-limit 36 \
    --quantize 4bit

# Adversarial testing (security research only)
python run_rev.py /path/to/model \
    --adversarial \
    --adversarial-ratio 0.2 \
    --adversarial-types divergence_attack mrcj \
    --challenges 10

# Comprehensive adversarial suite with safety wrappers
python run_rev.py /path/to/model \
    --adversarial \
    --adversarial-suite \
    --include-dangerous \
    --output adversarial_results.json
```

#### CLI Options

**Core Features:**
- `--challenges N`: Number of PoT challenges (default: 5)
- `--challenge-focus`: Focus strategy: coverage/separation/balanced
- `--max-tokens`: Maximum tokens to generate (default: 50)

**Advanced Analysis:**
- `--profiler`: Enable 16-dimensional behavioral profiling
- `--cassettes`: Enable Phase 2 cassette-based analysis
- `--cassette-topology`: Path to topology JSON
- `--cassette-types`: Probe types to include
- `--cassette-output`: Output directory for cassette results

**Adversarial Testing (Security Research Only):**
- `--adversarial`: Enable adversarial prompt generation
- `--adversarial-ratio`: Ratio of adversarial prompts (default: 0.1)
- `--adversarial-types`: Specific attack types to use
  - `divergence_attack`: 150x faster extraction
  - `mrcj`: Multi-round conversational jailbreaking
  - `special_char`: Special character triggers
  - `two_stage_inversion`: Model inversion attacks
  - `spv_mia`: Membership inference probes
  - `alignment_faking`: Alignment faking detection
  - `pair_algorithm`: PAIR algorithm jailbreaks
  - `cross_lingual`: Cross-lingual attacks
  - `temperature_exploit`: Temperature exploitation
  - `dataset_extraction`: Dataset extraction probes
  - `deception_pattern`: Deception pattern detection
- `--adversarial-suite`: Generate comprehensive adversarial suite
- `--include-dangerous`: Include high-risk prompts (with safety wrappers)

**Feature Toggles:**
- `--no-behavioral`: Disable behavioral analysis
- `--no-pot`: Disable PoT challenges
- `--no-validation`: Skip paper claims validation

**System Options:**
- `--local`: Enable local model loading
- `--device`: Device for computation (auto/cpu/cuda/mps)
- `--memory-limit`: Memory limit in GB
- `--quantize`: Quantization level (none/8bit/4bit)
- `--debug`: Enable debug logging
- `--output`: Output file for results

## üó∫Ô∏è Behavioral Topology Format

The topology file captures the model's computational structure:

```json
{
  "restriction_sites": [
    {"layer": 1, "percent_change": 32.8, "divergence_delta": 0.103}
  ],
  "stable_regions": [
    {"start": 4, "end": 16, "layers": 13, "std_dev": 0.0063}
  ],
  "optimization_hints": {
    "parallel_safe_regions": [
      {"layers": [4,5,6...16], "recommended_workers": 11}
    ]
  }
}
```

### Benefits of Topology-Aware Execution

| Metric | Without Topology | With Topology | Improvement |
|--------|-----------------|---------------|-------------|
| First profiling | 43 hours | 35 hours | 1.2x |
| Re-verification | 43 hours | **4 hours** | **10.8x** |
| Memory efficiency | Sequential | Parallel batches | 11x throughput |
| Model comparison | Full scan | Target boundaries | 5x faster |

## üìà Live Experiment Status - 50% MILESTONE

**80-Layer Behavioral Profiling of Llama 3.3 70B**
- **Progress**: Layer 39 of 80 (48.8% complete!)
- **Discovered**: 4 distinct behavioral phases with 16-layer symmetry
- **Memory**: 2GB stable (98.5% reduction from 131GB)
- **Key Finding**: Four-phase architecture validates natural segmentation

### üî¨ Four-Phase Architecture Discovered

| Phase | Layers | Avg Divergence | Description |
|-------|--------|---------------|-------------|
| 1 | 0-3 | 0.416 | Embedding/tokenization |
| 2 | 4-19 | 0.508 | Stable feature extraction |
| 3 | 20-35 | 0.527 | Deep semantic processing |
| 4 | 36-39+ | 0.535 | Emerging specialization |

See [FOUR_PHASE_ARCHITECTURE.md](FOUR_PHASE_ARCHITECTURE.md) for detailed analysis.

Monitor progress: `python monitor_80layers.py`

## üî¨ Technical Details

### PoT Challenge Generation

Sophisticated, discriminative prompts based on Proof-of-Thought methodology:

```python
from src.challenges.pot_challenge_generator import PoTChallengeGenerator

generator = PoTChallengeGenerator()

# Generate behavioral probes
probes = generator.generate_behavioral_probes()
# Returns: boundary, computation, reasoning, theoretical challenges

# Generate verification challenges with trade-off control
challenges = generator.generate_verification_challenges(
    n=10,
    focus="balanced"  # or "coverage" or "separation"
)
```

### Information-Theoretic Divergence

Multi-component behavioral analysis:

```python
divergence = (
    0.25 * coefficient_of_variation +  # Activation complexity
    0.25 * inter_layer_dynamics +       # Layer interactions
    0.20 * sparsity_patterns +          # Structural diversity
    0.15 * dynamic_range +              # Value distributions
    0.15 * information_entropy          # Information content
)
```

### Diagnostic Monitoring

Comprehensive execution tracking:

```python
from src.diagnostics.probe_monitor import ProbeMonitor

monitor = ProbeMonitor()

# Track probe execution
record = monitor.track_probe_execution(
    probe_text="Complex PoT challenge...",
    layer_idx=4,
    execution_time_ms=442.8,
    divergence_score=0.312
)

# Generate diagnostic report
report = monitor.generate_report()
```

## üõ†Ô∏è Development

### Running Tests

```bash
# Full test suite
pytest tests/

# Specific components
pytest tests/test_pot_challenges.py      # PoT generation
pytest tests/test_behavioral_sites.py    # Behavioral analysis
pytest tests/test_diagnostics.py         # Monitoring system
pytest tests/test_memory_bounded.py      # Memory management

# With coverage
pytest --cov=src --cov-report=html
```

### Code Quality

```bash
# Format code
black src/ tests/

# Type checking
mypy src/

# Linting
flake8 src/
```

## üìä Experimental Results

### Memory-Bounded Execution

Successfully executed models far exceeding device memory:

| Model | Size | Device RAM | Success | Time |
|-------|------|------------|---------|------|
| Llama 3.3 70B | 131.4GB | 64GB | ‚úÖ | ~24h |
| Yi-34B | 68GB | 64GB | ‚úÖ | 21min |
| Llama 2 7B | 13GB | 8GB | ‚úÖ | 8min |

### Behavioral Discrimination

Reliable model differentiation through behavioral analysis:

| Comparison | Divergence | Decision | Confidence |
|------------|------------|----------|------------|
| Llama vs GPT | 0.452 | DIFFERENT | 99.8% |
| Llama vs Llama (same) | 0.008 | SAME | 99.9% |
| Yi-34B vs Llama-70B | 0.387 | DIFFERENT | 99.6% |

## üîí Security & Privacy

### Cryptographic Verification
- Merkle trees for segment commitments
- SHA256 signatures for integrity
- Byzantine consensus for validation

### Privacy Preservation
- Hypervector obfuscation
- Optional homomorphic operations
- Differential privacy support

### ‚ö†Ô∏è Adversarial Testing Capabilities

**IMPORTANT**: The adversarial generation features are for authorized security research ONLY.

REV includes sophisticated adversarial prompt generation for model robustness testing:

**Attack Categories:**
1. **Extraction Attacks**: Divergence Attack (150x faster), dataset extraction
2. **Jailbreaking**: MRCJ (>90% success), PAIR algorithm, special characters
3. **Inference Attacks**: SPV-MIA, two-stage inversion (38-75% success)
4. **Analysis Tools**: Alignment faking detection, deception patterns

**Safety Controls:**
- All dangerous prompts wrapped with research context
- Warning labels on high-risk prompts
- Research-only flags in metadata
- Opt-in required for dangerous generation (`--include-dangerous`)

See [ADVERSARIAL_CAPABILITIES.md](ADVERSARIAL_CAPABILITIES.md) for detailed documentation and ethical guidelines.

## üìö Documentation

- [Full Technical Paper](docs/Restriction%20Enzyme%20Verification%20(REV)%20for%20Memory-Bounded,%20Black-Box%20LLM%20Comparison.md)
- [Yi-34B Validation Report](YI34B_EXPERIMENT_REPORT.md)
- [API Documentation](docs/api.md)
- [Architecture Guide](docs/architecture.md)

## ü§ù Contributing

We welcome contributions! Key areas for development:
- Optimizing segment execution speed
- Expanding model architecture support
- Improving diagnostic visualizations
- Adding more PoT challenge categories

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

## üôè Acknowledgments

REV builds upon:
- Transformer mechanistic interpretability research
- Hyperdimensional computing from GenomeVault
- Byzantine fault-tolerant consensus protocols
- Sequential statistical testing theory
- Proof-of-Thought (PoT) verification methodology

## üìû Contact

- **Repository**: [github.com/rohanvinaik/REV](https://github.com/rohanvinaik/REV)
- **Issues**: [GitHub Issues](https://github.com/rohanvinaik/REV/issues)
- **Discussions**: [GitHub Discussions](https://github.com/rohanvinaik/REV/discussions)

---

**Status**: Production Ready | **Version**: 3.0 | **Last Updated**: September 2, 2025