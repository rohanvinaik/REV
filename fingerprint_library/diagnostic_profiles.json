{
  "scan_metadata": {
    "timestamp": "2025-09-02T18:53:37.437604",
    "models_dir": "/Users/rohanvinaik/LLM_models",
    "num_architectures": 11
  },
  "architectures": {
    "gpt2": {
      "architecture_family": "gpt2",
      "model_name": "gpt2",
      "model_path": "/Users/rohanvinaik/LLM_models/gpt2",
      "num_layers": 12,
      "hidden_size": 768,
      "num_attention_heads": 12,
      "intermediate_size": 0,
      "vocab_size": 50257,
      "max_position_embeddings": 1024,
      "activation_function": "gelu_new",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "dd6abe69042ee19b",
      "scan_timestamp": "2025-09-02T18:53:37.425803",
      "file_size_gb": 0.5104627190157771
    },
    "gpt-neo": {
      "architecture_family": "gpt-neo",
      "model_name": "gpt-neo-125m",
      "model_path": "/Users/rohanvinaik/LLM_models/gpt-neo-125m",
      "num_layers": 0,
      "hidden_size": 768,
      "num_attention_heads": 0,
      "intermediate_size": null,
      "vocab_size": 50257,
      "max_position_embeddings": 2048,
      "activation_function": "gelu_new",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "847b917152fce49f",
      "scan_timestamp": "2025-09-02T18:53:37.427061",
      "file_size_gb": 0.4664169028401375
    },
    "pythia": {
      "architecture_family": "pythia",
      "model_name": "models--EleutherAI--pythia-70m",
      "model_path": "/Users/rohanvinaik/LLM_models/models--EleutherAI--pythia-70m",
      "num_layers": 6,
      "hidden_size": 512,
      "num_attention_heads": 8,
      "intermediate_size": 2048,
      "vocab_size": 50304,
      "max_position_embeddings": 2048,
      "activation_function": "gelu",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "00b111e9101c9990",
      "scan_timestamp": "2025-09-02T18:53:37.427797",
      "file_size_gb": 0.15462734922766685
    },
    "llama": {
      "architecture_family": "llama",
      "model_name": "llama-2-7b-hf",
      "model_path": "/Users/rohanvinaik/LLM_models/llama-2-7b-hf",
      "num_layers": 32,
      "hidden_size": 4096,
      "num_attention_heads": 32,
      "intermediate_size": 11008,
      "vocab_size": 32000,
      "max_position_embeddings": 4096,
      "activation_function": "silu",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "unknown",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "6662c77d27802b7c",
      "scan_timestamp": "2025-09-02T18:53:37.428866",
      "file_size_gb": 37.65398405585438
    },
    "falcon": {
      "architecture_family": "falcon",
      "model_name": "falcon-7b",
      "model_path": "/Users/rohanvinaik/LLM_models/falcon-7b",
      "num_layers": 32,
      "hidden_size": 4544,
      "num_attention_heads": 71,
      "intermediate_size": 0,
      "vocab_size": 65024,
      "max_position_embeddings": 0,
      "activation_function": "gelu",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "25850a0728f3d414",
      "scan_timestamp": "2025-09-02T18:53:37.429795",
      "file_size_gb": 26.886214341968298
    },
    "mistral": {
      "architecture_family": "mistral",
      "model_name": "mistral_for_colab",
      "model_path": "/Users/rohanvinaik/LLM_models/mistral_for_colab",
      "num_layers": 32,
      "hidden_size": 4096,
      "num_attention_heads": 32,
      "intermediate_size": 14336,
      "vocab_size": 32768,
      "max_position_embeddings": 32768,
      "activation_function": "silu",
      "attention_type": "grouped_query",
      "has_bias": false,
      "layer_norm_type": "unknown",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "1fd1561ea0c09f65",
      "scan_timestamp": "2025-09-02T18:53:37.430933",
      "file_size_gb": 27.00105257332325
    },
    "phi": {
      "architecture_family": "phi",
      "model_name": "phi-2",
      "model_path": "/Users/rohanvinaik/LLM_models/phi-2",
      "num_layers": 32,
      "hidden_size": 2560,
      "num_attention_heads": 32,
      "intermediate_size": 10240,
      "vocab_size": 51200,
      "max_position_embeddings": 2048,
      "activation_function": "gelu_new",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "9d6fe556a1778ed8",
      "scan_timestamp": "2025-09-02T18:53:37.431862",
      "file_size_gb": 5.17761092633009
    },
    "yi": {
      "architecture_family": "yi",
      "model_name": "yi-34b",
      "model_path": "/Users/rohanvinaik/LLM_models/yi-34b",
      "num_layers": 60,
      "hidden_size": 7168,
      "num_attention_heads": 56,
      "intermediate_size": 20480,
      "vocab_size": 64000,
      "max_position_embeddings": 4096,
      "activation_function": "silu",
      "attention_type": "grouped_query",
      "has_bias": false,
      "layer_norm_type": "unknown",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "635f04e1e46bb593",
      "scan_timestamp": "2025-09-02T18:53:37.433004",
      "file_size_gb": 128.10893427859992
    },
    "vicuna": {
      "architecture_family": "vicuna",
      "model_name": "vicuna-7b-v1.5",
      "model_path": "/Users/rohanvinaik/LLM_models/vicuna-7b-v1.5",
      "num_layers": 32,
      "hidden_size": 4096,
      "num_attention_heads": 32,
      "intermediate_size": 11008,
      "vocab_size": 32000,
      "max_position_embeddings": 4096,
      "activation_function": "silu",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "unknown",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "6662c77d27802b7c",
      "scan_timestamp": "2025-09-02T18:53:37.434288",
      "file_size_gb": 12.551387862302363
    },
    "zephyr": {
      "architecture_family": "zephyr",
      "model_name": "zephyr-7b-beta-final",
      "model_path": "/Users/rohanvinaik/LLM_models/zephyr-7b-beta-final",
      "num_layers": 32,
      "hidden_size": 4096,
      "num_attention_heads": 32,
      "intermediate_size": 14336,
      "vocab_size": 32000,
      "max_position_embeddings": 32768,
      "activation_function": "silu",
      "attention_type": "grouped_query",
      "has_bias": false,
      "layer_norm_type": "unknown",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "1fd1561ea0c09f65",
      "scan_timestamp": "2025-09-02T18:53:37.435022",
      "file_size_gb": 26.977676523849368
    },
    "dialogpt": {
      "architecture_family": "dialogpt",
      "model_name": "models--microsoft--DialoGPT-small",
      "model_path": "/Users/rohanvinaik/LLM_models/models--microsoft--DialoGPT-small",
      "num_layers": 12,
      "hidden_size": 768,
      "num_attention_heads": 12,
      "intermediate_size": 0,
      "vocab_size": 50257,
      "max_position_embeddings": 1024,
      "activation_function": "gelu_new",
      "attention_type": "multi_head",
      "has_bias": false,
      "layer_norm_type": "pre",
      "residual_pattern": "standard",
      "rope_scaling": null,
      "has_vision": false,
      "is_mixture_of_experts": false,
      "num_experts": null,
      "structural_hash": "dd6abe69042ee19b",
      "scan_timestamp": "2025-09-02T18:53:37.436907",
      "file_size_gb": 0.32713319920003414
    }
  }
}